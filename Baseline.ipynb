{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayakpaul/Generalized-ODIN-TF/blob/main/Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFwWgoioRXy9"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCQ1S81KIbaN",
    "outputId": "5a9b4b37-56fd-4665-b754-a0e267a094d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User name: sayakpaul\n",
      "Password: ··········\n",
      "Repo Address: sayakpaul/Generalized-ODIN-TF\n",
      "Branch name: main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "from getpass import getpass\n",
    "\n",
    "user = input('User name: ')\n",
    "password = getpass('Password: ')\n",
    "password = urllib.parse.quote(password)\n",
    "repo_address = input('Repo Address: ')\n",
    "branch_name = input('Branch name: ')\n",
    "\n",
    "cmd_string = 'git clone https://{}:{}@github.com/{}.git -b {}'.format(\n",
    "    user, password, repo_address, branch_name\n",
    ")\n",
    "\n",
    "os.system(cmd_string)\n",
    "cmd_string, password = \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QtwAr7vgIpS5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"Generalized-ODIN-TF\")\n",
    "\n",
    "from scripts import resnet20\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWpj-WTpRZvS"
   },
   "source": [
    "## Load CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6mPJL_5JDsO",
    "outputId": "500c0fc7-dd46-45ae-9245-c2ae4ef6111a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 50000\n",
      "Total test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "print(f\"Total training examples: {len(x_train)}\")\n",
    "print(f\"Total test examples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMIkjg6_RbYc"
   },
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5A1ZdFW6J4_R"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "START_LR = 0.1\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtQQLiGNRdvx"
   },
   "source": [
    "## Prepare data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GqNtOj8yKO_X"
   },
   "outputs": [],
   "source": [
    "# Augmentation pipeline\n",
    "simple_aug = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
    "        layers.experimental.preprocessing.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now, map the augmentation pipeline to our training dataset\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(lambda x, y: (simple_aug(x), y), num_parallel_calls=AUTO)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "# Test dataset\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXUFugU3Riph"
   },
   "source": [
    "## Define LR schedule, optimizer, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8a6VIseo7GFP"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    if epoch < int(EPOCHS * 0.5) - 1:\n",
    "        return START_LR\n",
    "    elif epoch < int(EPOCHS*0.75) -1:\n",
    "        return float(START_LR * 0.1)\n",
    "    else:\n",
    "        return float(START_LR * 0.01)\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr_schedule(epoch), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iUhtb9ZiM17B"
   },
   "outputs": [],
   "source": [
    "# Optimizer and loss function.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=START_LR, momentum=0.9)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkU9afGCRnaC"
   },
   "source": [
    "## Model training with ResNet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3zhJ1iFPLzp",
    "outputId": "a858aef4-4c17-4b90-cc10-413707d2cbff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 24ms/step - loss: 2.5846 - accuracy: 0.3279 - val_loss: 2.5704 - val_accuracy: 0.3266\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.6942 - accuracy: 0.5531 - val_loss: 2.2108 - val_accuracy: 0.4579\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.3634 - accuracy: 0.6318 - val_loss: 1.4847 - val_accuracy: 0.5930\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.2164 - accuracy: 0.6688 - val_loss: 1.8642 - val_accuracy: 0.5198\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1380 - accuracy: 0.6938 - val_loss: 1.4239 - val_accuracy: 0.6365\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.0848 - accuracy: 0.7154 - val_loss: 1.7006 - val_accuracy: 0.5369\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.0571 - accuracy: 0.7249 - val_loss: 1.8742 - val_accuracy: 0.5254\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.0368 - accuracy: 0.7335 - val_loss: 1.2454 - val_accuracy: 0.6664\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.0236 - accuracy: 0.7411 - val_loss: 1.2918 - val_accuracy: 0.6521\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.0086 - accuracy: 0.7494 - val_loss: 1.0956 - val_accuracy: 0.7249\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.0062 - accuracy: 0.7492 - val_loss: 1.9737 - val_accuracy: 0.5561\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9890 - accuracy: 0.7568 - val_loss: 1.7617 - val_accuracy: 0.5819\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9855 - accuracy: 0.7597 - val_loss: 2.4520 - val_accuracy: 0.4894\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9805 - accuracy: 0.7656 - val_loss: 1.6390 - val_accuracy: 0.5967\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9644 - accuracy: 0.7694 - val_loss: 1.2896 - val_accuracy: 0.6798\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9632 - accuracy: 0.7716 - val_loss: 1.1531 - val_accuracy: 0.7071\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9627 - accuracy: 0.7747 - val_loss: 1.2946 - val_accuracy: 0.6971\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9691 - accuracy: 0.7724 - val_loss: 1.2848 - val_accuracy: 0.6598\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9550 - accuracy: 0.7784 - val_loss: 1.2605 - val_accuracy: 0.6845\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9591 - accuracy: 0.7760 - val_loss: 1.3766 - val_accuracy: 0.6293\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9505 - accuracy: 0.7805 - val_loss: 2.1374 - val_accuracy: 0.5344\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9480 - accuracy: 0.7814 - val_loss: 1.1982 - val_accuracy: 0.7025\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9445 - accuracy: 0.7860 - val_loss: 1.3803 - val_accuracy: 0.6756\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9569 - accuracy: 0.7807 - val_loss: 1.2924 - val_accuracy: 0.6892\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9506 - accuracy: 0.7826 - val_loss: 1.2555 - val_accuracy: 0.7017\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9424 - accuracy: 0.7871 - val_loss: 1.1413 - val_accuracy: 0.7312\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9484 - accuracy: 0.7883 - val_loss: 1.1325 - val_accuracy: 0.7195\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9344 - accuracy: 0.7904 - val_loss: 1.5787 - val_accuracy: 0.6066\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9254 - accuracy: 0.7927 - val_loss: 1.4476 - val_accuracy: 0.6395\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9335 - accuracy: 0.7922 - val_loss: 1.8803 - val_accuracy: 0.5965\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9426 - accuracy: 0.7880 - val_loss: 1.9555 - val_accuracy: 0.5716\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9325 - accuracy: 0.7923 - val_loss: 1.4505 - val_accuracy: 0.6557\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9327 - accuracy: 0.7934 - val_loss: 1.4019 - val_accuracy: 0.6650\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9299 - accuracy: 0.7912 - val_loss: 1.2768 - val_accuracy: 0.6886\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9251 - accuracy: 0.7954 - val_loss: 1.5965 - val_accuracy: 0.6094\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9341 - accuracy: 0.7943 - val_loss: 1.5342 - val_accuracy: 0.6408\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9202 - accuracy: 0.7972 - val_loss: 1.2290 - val_accuracy: 0.7068\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9251 - accuracy: 0.7987 - val_loss: 2.3307 - val_accuracy: 0.5627\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9267 - accuracy: 0.7961 - val_loss: 1.6748 - val_accuracy: 0.5868\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9362 - accuracy: 0.7956 - val_loss: 1.6976 - val_accuracy: 0.6218\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9346 - accuracy: 0.7974 - val_loss: 1.3160 - val_accuracy: 0.6775\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9293 - accuracy: 0.7971 - val_loss: 1.3233 - val_accuracy: 0.6988\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9262 - accuracy: 0.7979 - val_loss: 1.9747 - val_accuracy: 0.5180\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9130 - accuracy: 0.8047 - val_loss: 1.9412 - val_accuracy: 0.5566\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9157 - accuracy: 0.8015 - val_loss: 2.0874 - val_accuracy: 0.5376\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9322 - accuracy: 0.7978 - val_loss: 4.8096 - val_accuracy: 0.3077\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9388 - accuracy: 0.7955 - val_loss: 1.2928 - val_accuracy: 0.6739\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9126 - accuracy: 0.8035 - val_loss: 1.6537 - val_accuracy: 0.6157\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9223 - accuracy: 0.8014 - val_loss: 1.4695 - val_accuracy: 0.6273\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9139 - accuracy: 0.8047 - val_loss: 1.3569 - val_accuracy: 0.6729\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9175 - accuracy: 0.8048 - val_loss: 1.2764 - val_accuracy: 0.7003\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9132 - accuracy: 0.8020 - val_loss: 1.3749 - val_accuracy: 0.6717\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9331 - accuracy: 0.7965 - val_loss: 1.4696 - val_accuracy: 0.6704\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9237 - accuracy: 0.8008 - val_loss: 1.4201 - val_accuracy: 0.6531\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9077 - accuracy: 0.8063 - val_loss: 1.5082 - val_accuracy: 0.6306\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9187 - accuracy: 0.8048 - val_loss: 1.5716 - val_accuracy: 0.6383\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9226 - accuracy: 0.8025 - val_loss: 1.3414 - val_accuracy: 0.6801\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9128 - accuracy: 0.8060 - val_loss: 1.2252 - val_accuracy: 0.7161\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9170 - accuracy: 0.8033 - val_loss: 1.4625 - val_accuracy: 0.6650\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9146 - accuracy: 0.8050 - val_loss: 2.0226 - val_accuracy: 0.6052\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9076 - accuracy: 0.8073 - val_loss: 1.2285 - val_accuracy: 0.7030\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9179 - accuracy: 0.8063 - val_loss: 1.5467 - val_accuracy: 0.6380\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9223 - accuracy: 0.8047 - val_loss: 1.2481 - val_accuracy: 0.7090\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9368 - accuracy: 0.7998 - val_loss: 1.3965 - val_accuracy: 0.6768\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9083 - accuracy: 0.8057 - val_loss: 2.6284 - val_accuracy: 0.5056\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9180 - accuracy: 0.8028 - val_loss: 2.4564 - val_accuracy: 0.4854\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9178 - accuracy: 0.8049 - val_loss: 1.6546 - val_accuracy: 0.5862\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9118 - accuracy: 0.8059 - val_loss: 1.6684 - val_accuracy: 0.6064\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9148 - accuracy: 0.8074 - val_loss: 1.3558 - val_accuracy: 0.6803\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9032 - accuracy: 0.8121 - val_loss: 1.3961 - val_accuracy: 0.6772\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9131 - accuracy: 0.8055 - val_loss: 1.2178 - val_accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9017 - accuracy: 0.8104 - val_loss: 1.4487 - val_accuracy: 0.6640\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9145 - accuracy: 0.8079 - val_loss: 2.7025 - val_accuracy: 0.4841\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9114 - accuracy: 0.8082 - val_loss: 1.4471 - val_accuracy: 0.6439\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9155 - accuracy: 0.8050 - val_loss: 1.3879 - val_accuracy: 0.6749\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9037 - accuracy: 0.8107 - val_loss: 4.4058 - val_accuracy: 0.3814\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9111 - accuracy: 0.8076 - val_loss: 2.2155 - val_accuracy: 0.5187\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9104 - accuracy: 0.8082 - val_loss: 1.3486 - val_accuracy: 0.6828\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9115 - accuracy: 0.8063 - val_loss: 1.2950 - val_accuracy: 0.6993\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9134 - accuracy: 0.8107 - val_loss: 1.4120 - val_accuracy: 0.6615\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9127 - accuracy: 0.8087 - val_loss: 1.8372 - val_accuracy: 0.5709\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9207 - accuracy: 0.8061 - val_loss: 4.8760 - val_accuracy: 0.3342\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9071 - accuracy: 0.8104 - val_loss: 1.3535 - val_accuracy: 0.7054\n",
      "Epoch 84/200\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9020 - accuracy: 0.8129 - val_loss: 2.7806 - val_accuracy: 0.4574\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9196 - accuracy: 0.8104 - val_loss: 1.3610 - val_accuracy: 0.6646\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9111 - accuracy: 0.8130 - val_loss: 2.4003 - val_accuracy: 0.4816\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9110 - accuracy: 0.8103 - val_loss: 1.7929 - val_accuracy: 0.5896\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9114 - accuracy: 0.8094 - val_loss: 1.6042 - val_accuracy: 0.6474\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9000 - accuracy: 0.8116 - val_loss: 1.3313 - val_accuracy: 0.6810\n",
      "Epoch 90/200\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9122 - accuracy: 0.8126 - val_loss: 1.3316 - val_accuracy: 0.6840\n",
      "Epoch 91/200\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9151 - accuracy: 0.8105 - val_loss: 1.4271 - val_accuracy: 0.6669\n",
      "Epoch 92/200\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9151 - accuracy: 0.8112 - val_loss: 1.3526 - val_accuracy: 0.6763\n",
      "Epoch 93/200\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9101 - accuracy: 0.8131 - val_loss: 1.2378 - val_accuracy: 0.7051\n",
      "Epoch 94/200\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9110 - accuracy: 0.8133 - val_loss: 2.2297 - val_accuracy: 0.5399\n",
      "Epoch 95/200\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9112 - accuracy: 0.8106 - val_loss: 2.7000 - val_accuracy: 0.4638\n",
      "Epoch 96/200\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9090 - accuracy: 0.8134 - val_loss: 1.8718 - val_accuracy: 0.5738\n",
      "Epoch 97/200\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9062 - accuracy: 0.8111 - val_loss: 2.1100 - val_accuracy: 0.5725\n",
      "Epoch 98/200\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.8962 - accuracy: 0.8163 - val_loss: 1.3535 - val_accuracy: 0.6970\n",
      "Epoch 99/200\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9074 - accuracy: 0.8117 - val_loss: 1.7111 - val_accuracy: 0.6315\n",
      "Epoch 100/200\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.8604 - accuracy: 0.8301 - val_loss: 0.8105 - val_accuracy: 0.8362\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.6827 - accuracy: 0.8827 - val_loss: 0.7451 - val_accuracy: 0.8527\n",
      "Epoch 102/200\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.6252 - accuracy: 0.8949 - val_loss: 0.7173 - val_accuracy: 0.8576\n",
      "Epoch 103/200\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5878 - accuracy: 0.9020 - val_loss: 0.6756 - val_accuracy: 0.8667\n",
      "Epoch 104/200\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5579 - accuracy: 0.9033 - val_loss: 0.6335 - val_accuracy: 0.8724\n",
      "Epoch 105/200\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5285 - accuracy: 0.9058 - val_loss: 0.6101 - val_accuracy: 0.8755\n",
      "Epoch 106/200\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.5060 - accuracy: 0.9088 - val_loss: 0.6481 - val_accuracy: 0.8562\n",
      "Epoch 107/200\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4832 - accuracy: 0.9122 - val_loss: 0.6457 - val_accuracy: 0.8527\n",
      "Epoch 108/200\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4679 - accuracy: 0.9138 - val_loss: 0.5912 - val_accuracy: 0.8738\n",
      "Epoch 109/200\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4493 - accuracy: 0.9172 - val_loss: 0.5899 - val_accuracy: 0.8638\n",
      "Epoch 110/200\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4400 - accuracy: 0.9172 - val_loss: 0.6400 - val_accuracy: 0.8534\n",
      "Epoch 111/200\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4188 - accuracy: 0.9203 - val_loss: 0.5907 - val_accuracy: 0.8638\n",
      "Epoch 112/200\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4118 - accuracy: 0.9213 - val_loss: 0.5547 - val_accuracy: 0.8734\n",
      "Epoch 113/200\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3997 - accuracy: 0.9238 - val_loss: 0.6375 - val_accuracy: 0.8494\n",
      "Epoch 114/200\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3982 - accuracy: 0.9199 - val_loss: 0.6019 - val_accuracy: 0.8581\n",
      "Epoch 115/200\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3962 - accuracy: 0.9215 - val_loss: 0.5753 - val_accuracy: 0.8663\n",
      "Epoch 116/200\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3881 - accuracy: 0.9229 - val_loss: 0.5260 - val_accuracy: 0.8772\n",
      "Epoch 117/200\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3945 - accuracy: 0.9197 - val_loss: 0.5327 - val_accuracy: 0.8761\n",
      "Epoch 118/200\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3850 - accuracy: 0.9225 - val_loss: 0.6322 - val_accuracy: 0.8503\n",
      "Epoch 119/200\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3802 - accuracy: 0.9234 - val_loss: 0.5556 - val_accuracy: 0.8698\n",
      "Epoch 120/200\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3787 - accuracy: 0.9224 - val_loss: 0.5085 - val_accuracy: 0.8820\n",
      "Epoch 121/200\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3852 - accuracy: 0.9198 - val_loss: 0.5811 - val_accuracy: 0.8594\n",
      "Epoch 122/200\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3727 - accuracy: 0.9249 - val_loss: 0.6800 - val_accuracy: 0.8416\n",
      "Epoch 123/200\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3738 - accuracy: 0.9244 - val_loss: 0.7380 - val_accuracy: 0.8139\n",
      "Epoch 124/200\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3795 - accuracy: 0.9217 - val_loss: 0.7285 - val_accuracy: 0.8278\n",
      "Epoch 125/200\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3746 - accuracy: 0.9263 - val_loss: 0.5669 - val_accuracy: 0.8644\n",
      "Epoch 126/200\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3729 - accuracy: 0.9239 - val_loss: 0.6425 - val_accuracy: 0.8385\n",
      "Epoch 127/200\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3739 - accuracy: 0.9237 - val_loss: 0.6643 - val_accuracy: 0.8403\n",
      "Epoch 128/200\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3695 - accuracy: 0.9236 - val_loss: 0.5778 - val_accuracy: 0.8649\n",
      "Epoch 129/200\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3708 - accuracy: 0.9251 - val_loss: 0.5455 - val_accuracy: 0.8692\n",
      "Epoch 130/200\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3687 - accuracy: 0.9270 - val_loss: 0.7762 - val_accuracy: 0.8220\n",
      "Epoch 131/200\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3647 - accuracy: 0.9269 - val_loss: 0.7622 - val_accuracy: 0.8140\n",
      "Epoch 132/200\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3700 - accuracy: 0.9273 - val_loss: 0.7014 - val_accuracy: 0.8346\n",
      "Epoch 133/200\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3758 - accuracy: 0.9230 - val_loss: 0.6809 - val_accuracy: 0.8443\n",
      "Epoch 134/200\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3696 - accuracy: 0.9274 - val_loss: 0.6210 - val_accuracy: 0.8533\n",
      "Epoch 135/200\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3634 - accuracy: 0.9285 - val_loss: 0.6024 - val_accuracy: 0.8584\n",
      "Epoch 136/200\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3640 - accuracy: 0.9279 - val_loss: 0.6775 - val_accuracy: 0.8359\n",
      "Epoch 137/200\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3683 - accuracy: 0.9281 - val_loss: 0.7173 - val_accuracy: 0.8355\n",
      "Epoch 138/200\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3692 - accuracy: 0.9279 - val_loss: 0.6122 - val_accuracy: 0.8555\n",
      "Epoch 139/200\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3733 - accuracy: 0.9266 - val_loss: 0.7534 - val_accuracy: 0.8311\n",
      "Epoch 140/200\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3694 - accuracy: 0.9271 - val_loss: 0.6842 - val_accuracy: 0.8378\n",
      "Epoch 141/200\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3580 - accuracy: 0.9319 - val_loss: 0.6829 - val_accuracy: 0.8397\n",
      "Epoch 142/200\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3664 - accuracy: 0.9287 - val_loss: 0.6804 - val_accuracy: 0.8371\n",
      "Epoch 143/200\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3634 - accuracy: 0.9307 - val_loss: 0.6924 - val_accuracy: 0.8388\n",
      "Epoch 144/200\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3659 - accuracy: 0.9283 - val_loss: 0.7907 - val_accuracy: 0.8215\n",
      "Epoch 145/200\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3677 - accuracy: 0.9273 - val_loss: 0.6233 - val_accuracy: 0.8589\n",
      "Epoch 146/200\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3622 - accuracy: 0.9325 - val_loss: 0.7036 - val_accuracy: 0.8394\n",
      "Epoch 147/200\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3630 - accuracy: 0.9310 - val_loss: 0.8380 - val_accuracy: 0.8193\n",
      "Epoch 148/200\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3708 - accuracy: 0.9290 - val_loss: 0.5574 - val_accuracy: 0.8755\n",
      "Epoch 149/200\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3659 - accuracy: 0.9328 - val_loss: 0.6165 - val_accuracy: 0.8565\n",
      "Epoch 150/200\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.3440 - accuracy: 0.9403 - val_loss: 0.4691 - val_accuracy: 0.8986\n",
      "Epoch 151/200\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2900 - accuracy: 0.9604 - val_loss: 0.4525 - val_accuracy: 0.9063\n",
      "Epoch 152/200\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2778 - accuracy: 0.9646 - val_loss: 0.4451 - val_accuracy: 0.9088\n",
      "Epoch 153/200\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2698 - accuracy: 0.9669 - val_loss: 0.4513 - val_accuracy: 0.9059\n",
      "Epoch 154/200\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2633 - accuracy: 0.9689 - val_loss: 0.4447 - val_accuracy: 0.9077\n",
      "Epoch 155/200\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2595 - accuracy: 0.9696 - val_loss: 0.4395 - val_accuracy: 0.9108\n",
      "Epoch 156/200\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2531 - accuracy: 0.9721 - val_loss: 0.4384 - val_accuracy: 0.9100\n",
      "Epoch 157/200\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2486 - accuracy: 0.9730 - val_loss: 0.4477 - val_accuracy: 0.9089\n",
      "Epoch 158/200\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2468 - accuracy: 0.9737 - val_loss: 0.4342 - val_accuracy: 0.9115\n",
      "Epoch 159/200\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2418 - accuracy: 0.9753 - val_loss: 0.4361 - val_accuracy: 0.9115\n",
      "Epoch 160/200\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2390 - accuracy: 0.9750 - val_loss: 0.4304 - val_accuracy: 0.9131\n",
      "Epoch 161/200\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2364 - accuracy: 0.9753 - val_loss: 0.4353 - val_accuracy: 0.9111\n",
      "Epoch 162/200\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2330 - accuracy: 0.9771 - val_loss: 0.4376 - val_accuracy: 0.9089\n",
      "Epoch 163/200\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2301 - accuracy: 0.9759 - val_loss: 0.4232 - val_accuracy: 0.9140\n",
      "Epoch 164/200\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2262 - accuracy: 0.9783 - val_loss: 0.4335 - val_accuracy: 0.9119\n",
      "Epoch 165/200\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2224 - accuracy: 0.9792 - val_loss: 0.4393 - val_accuracy: 0.9108\n",
      "Epoch 166/200\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2224 - accuracy: 0.9790 - val_loss: 0.4341 - val_accuracy: 0.9121\n",
      "Epoch 167/200\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2173 - accuracy: 0.9800 - val_loss: 0.4369 - val_accuracy: 0.9098\n",
      "Epoch 168/200\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2156 - accuracy: 0.9805 - val_loss: 0.4276 - val_accuracy: 0.9132\n",
      "Epoch 169/200\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2131 - accuracy: 0.9812 - val_loss: 0.4282 - val_accuracy: 0.9134\n",
      "Epoch 170/200\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2119 - accuracy: 0.9803 - val_loss: 0.4268 - val_accuracy: 0.9138\n",
      "Epoch 171/200\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2084 - accuracy: 0.9825 - val_loss: 0.4242 - val_accuracy: 0.9131\n",
      "Epoch 172/200\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2090 - accuracy: 0.9804 - val_loss: 0.4263 - val_accuracy: 0.9128\n",
      "Epoch 173/200\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2050 - accuracy: 0.9833 - val_loss: 0.4315 - val_accuracy: 0.9130\n",
      "Epoch 174/200\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2066 - accuracy: 0.9809 - val_loss: 0.4292 - val_accuracy: 0.9121\n",
      "Epoch 175/200\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2024 - accuracy: 0.9819 - val_loss: 0.4206 - val_accuracy: 0.9159\n",
      "Epoch 176/200\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2025 - accuracy: 0.9813 - val_loss: 0.4210 - val_accuracy: 0.9138\n",
      "Epoch 177/200\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.2002 - accuracy: 0.9824 - val_loss: 0.4123 - val_accuracy: 0.9156\n",
      "Epoch 178/200\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1982 - accuracy: 0.9826 - val_loss: 0.4206 - val_accuracy: 0.9140\n",
      "Epoch 179/200\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1969 - accuracy: 0.9819 - val_loss: 0.4192 - val_accuracy: 0.9149\n",
      "Epoch 180/200\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1915 - accuracy: 0.9854 - val_loss: 0.4379 - val_accuracy: 0.9102\n",
      "Epoch 181/200\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1932 - accuracy: 0.9832 - val_loss: 0.4164 - val_accuracy: 0.9138\n",
      "Epoch 182/200\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1867 - accuracy: 0.9854 - val_loss: 0.4321 - val_accuracy: 0.9109\n",
      "Epoch 183/200\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1876 - accuracy: 0.9840 - val_loss: 0.4181 - val_accuracy: 0.9153\n",
      "Epoch 184/200\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1864 - accuracy: 0.9835 - val_loss: 0.4215 - val_accuracy: 0.9133\n",
      "Epoch 185/200\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1844 - accuracy: 0.9851 - val_loss: 0.4244 - val_accuracy: 0.9127\n",
      "Epoch 186/200\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1864 - accuracy: 0.9840 - val_loss: 0.4184 - val_accuracy: 0.9125\n",
      "Epoch 187/200\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1803 - accuracy: 0.9865 - val_loss: 0.4285 - val_accuracy: 0.9113\n",
      "Epoch 188/200\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1791 - accuracy: 0.9861 - val_loss: 0.4250 - val_accuracy: 0.9122\n",
      "Epoch 189/200\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1781 - accuracy: 0.9868 - val_loss: 0.4131 - val_accuracy: 0.9160\n",
      "Epoch 190/200\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1770 - accuracy: 0.9857 - val_loss: 0.4106 - val_accuracy: 0.9136\n",
      "Epoch 191/200\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1774 - accuracy: 0.9848 - val_loss: 0.4216 - val_accuracy: 0.9120\n",
      "Epoch 192/200\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1749 - accuracy: 0.9866 - val_loss: 0.4140 - val_accuracy: 0.9140\n",
      "Epoch 193/200\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1718 - accuracy: 0.9864 - val_loss: 0.4272 - val_accuracy: 0.9095\n",
      "Epoch 194/200\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1709 - accuracy: 0.9866 - val_loss: 0.4096 - val_accuracy: 0.9151\n",
      "Epoch 195/200\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1704 - accuracy: 0.9867 - val_loss: 0.4077 - val_accuracy: 0.9153\n",
      "Epoch 196/200\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1688 - accuracy: 0.9868 - val_loss: 0.4143 - val_accuracy: 0.9160\n",
      "Epoch 197/200\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1688 - accuracy: 0.9871 - val_loss: 0.4161 - val_accuracy: 0.9114\n",
      "Epoch 198/200\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1654 - accuracy: 0.9874 - val_loss: 0.4303 - val_accuracy: 0.9103\n",
      "Epoch 199/200\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1662 - accuracy: 0.9860 - val_loss: 0.4022 - val_accuracy: 0.9182\n",
      "Epoch 200/200\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1626 - accuracy: 0.9872 - val_loss: 0.4087 - val_accuracy: 0.9146\n"
     ]
    }
   ],
   "source": [
    "rn_model = tf.keras.models.load_model(\"initial_model\")\n",
    "rn_model.compile(loss=loss_fn, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = rn_model.fit(train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "QPWAgdc65Ppc",
    "outputId": "24617f95-2bf1-4e61-a469-ae0589a4441d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABJcklEQVR4nO2deZgcVbn/P6f32fdMJslkhewbJIGwJhEJO4i4IKC4XJGrcrl6UeDnAur16hVEBBVFQLmgIIIisi9mSFgDCQkkJBCy75mZzNbTM9Pb+f1xqqare7p7uicz09XD+TzPPNVdXV31dvX0t976nvecI6SUaDQajca+OHIdgEaj0WjSo4Vao9FobI4Wao1Go7E5Wqg1Go3G5mih1mg0GpvjGoqdVldXy4kTJw7ovZ2dnRQVFQ1uQIOAjit77Bqbjis7dFzZM5DY1qxZ0ySlrEn6opRy0P8WLFggB8qKFSsG/N6hRMeVPXaNTceVHTqu7BlIbMCbMoWmZpRRCyF2AB1ABAhLKRdmdanQaDQazYDJxvpYJqVsGrJINBqNRpMU3Zio0Wg0NkfIDLqQCyG2Ay2ABH4npbwzyTZXAFcA1NbWLnjwwQcHFJDf76e4uHhA7x1KdFzZY9fYdFzZkRiXEIKioiKcTmcOo1Lta0KInMaQinSxRSIROjs7SdTeZcuWrUlpK6cyr61/wBhjOQpYD5yabnvdmDh82DUuKe0bm44rOxLj2rZtm2xsbJTRaDQ3ARm0t7fn9PjpSBVbNBqVjY2Nctu2bX1eI01jYkbWh5Ryn7E8BPwdOC6T92k0mpFHd3c3VVVVts1m7YwQgqqqKrq7u7N6X79CLYQoEkKUmI+B5cCGAUWp0WhGBFqkB85Azl0mGXUt8JIQYj2wGnhCSvl01kfSaBLZ+HcIHM51FBqN7elXqKWU26SU84y/WVLKHw9HYJoRTlcL/PXzsOGRXEeiySNaW1v5zW9+M6D3nn322bS2tma8/Y033sjNN988oGMNNro8T5MbIiFjGcxtHJq8Ip1QRyKRtO998sknKS8vH4Kohh4t1HYmEoYnvwVte3MdyeATjcQvNZoMuO6669i6dSvz58/nu9/9Lg0NDSxbtoxLLrmEOXPmAPCxj32MBQsWMGvWLO68M1ZJPHHiRJqamtixYwczZszgy1/+MrNmzWL58uV0dXWlPe66detYvHgxc+fO5cILL6SlpQWA2267jZkzZzJ37lwuvvhiAF588UVOOukk5s+fzzHHHENHR8cRf+4hGZRJM0i07YLVd8KYY2D+JbmOZnCRhkDLaG7j0BwRP/jnRt7d1z6o+5w5ppQbzpuV9LWf/vSnbNiwgXXr1tHR0cGaNWtYvXo1GzZsYNKkSQDcc889VFZW0tXVxaJFi7jooouoqqqK28+WLVt44IEH+P3vf8+nPvUpHnnkES677LKUMX3uc5/j9ttvZ8mSJXz/+9/nBz/4Abfeeis//elP2b59O16vt9dWufnmm/n5z3/O6aefjt/vx+fzHfE50Rm1nRnJWaf5meQI/GyaYeW4447rFWlQWe68efNYvHgxu3fvZsuWLX3eM2nSJObPnw/AggUL2LFjR8r9t7W10draypIlSwC4/PLLWblyJQBz587l0ksv5f7778flUnnvSSedxPXXX89tt91Ga2tr7/ojQWfUdiY6grNOU6CjI/CzfYhIlfkOJ9bhRBsaGnj++ed59dVXKSwsZOnSpUlrlr1eb+9jp9PZr/WRiieeeIKVK1fy2GOP8aMf/YiNGzdy3XXXsXTpUl588UUWL17M888/z/Tp0we0fxOdUduZaFgtR2LWaQr0SLwIaYaMkpKStJ5vW1sbFRUVFBYWsnnzZl577bUjPmZZWRkVFRWsWrUKgPvuu48lS5YQjUbZvXs3y5Yt42c/+xmtra34/X62bt3KrFmzuPbaa1m4cCGbN28+4hh0Rm1n5Ai2PqS2PjTZU1VVxUknncTs2bM57bTTuPDCC+NeP/PMM/ntb3/L3LlzmTZtGosXLx6U4957771ceeWVBAIBJk+ezB/+8AcikQiXXXYZbW1tSCn5xje+QXl5Od/73vd44YUXcLvdzJw5k7POOuuIj6+F2s70ZtT9D5yVd4xk/10zpPz5z38GoKOjg5KSEpYuXdr7mtfr5amnnkr6PtOHrq6uZsOGWOfqa665Jun2N954Y+/j+fPnJ83OX3rppT7rbr/99t7YBgttfdiZXntgBIqZrvrQaDJGC7WdMTPqkZh16qoPjSZjtFDbmZGcdWrrQ6PJGC3UdmYkZ529F6ER6L9rNIOMFmo7o60PjUaDFmp7Y1oeIzHrHMmlhxrNIKOF2s6M6A4vI9h/1wwZRzLMKcCtt95KIBBI+trSpUt58803B7zvoUQLtZ0ZyQ1uusOLZgAMpVDbGS3UdqY3ox6BWadZIz4SL0KaISNxmFOAm266iUWLFjF37lxuuOEGADo7OznnnHOYN28es2fP5i9/+Qu33XYb+/btY9myZSxbtiztcR544AHmzJnD7NmzufbaawE13vXnP/95Zs+ezZw5c/jFL34BJB/qdLDRPRPtzEjOOnXVx8jgqevgwDuDu8/Rc+CsnyZ9KXGY02effZYtW7awevVqpJScf/75rFy5ksbGRsaMGcMTTzwBqDFAysrKuOWWW1ixYgXV1dUpD79v3z6uvfZa1qxZQ0VFBcuXL+fRRx+lvr6evXv39vZqNIc1TTbU6WCjM2o7k69ZZ1cLPPQ5XKE0A6brqg/NIPDss8/y7LPPcswxx3DssceyefNmtmzZwpw5c3j++ee59tprWbVqFWVlZRnv84033mDp0qXU1NTgcrm49NJLWblyJZMnT2bbtm1cddVVPP3005SWlgLJhzodbHRGbWfy1frY/za8+w+K5y1IvY2u+hgZpMh8hwspJddffz1f+cpX+ry2Zs0annzySa6//nqWL1/O97///Yz3mYyKigrWr1/PM888w69//Wseeugh7rnnnqRDnQ42OqO2M/naM9GIW6TLlnXVh2YAJA5zesYZZ3DPPffg9/sB2Lt3L4cOHWLfvn0UFhZy2WWXcc0117B27dqk70/G8ccfz4svvkhTUxORSIQHHniAJUuW0NTURDQa5aKLLuJHP/oRa9euTTnU6WCjM2o7k68dXgzLxpEu7pHsv2uGjMRhTn/5y1+yadMmTjjhBACKi4u5//77+eCDD/jWt76Fw+HA7XZzxx13AHDFFVdw1llnUVdXx4oVK5Ieo66ujp/85CcsW7YMKSVnn302F1xwAevXr+cLX/gCUeP/+yc/+UnKoU4HY55EK1qo7Uy+Zp0ZZdR56r9rco51mFOAq6++mquvvjpumylTpnDGGWf0ee9VV13FVVddlXS/DQ0NvY8vueQSLrkkfp7SefPm9WbmVpINdTrYaOvDzuRrg1s0A6HOV1tHo8kBWqjtTL42uGWUUedpQ6lGkwO0UNuZfLU+Msmo8/WzaYDUlRGa/hnIudNCbWfyNevMJKPO17sFDT6fj+bmZi3WA0BKSXNzMz6fL6v36cZEO5OvYmY0FIp0F5h89d81jBs3jj179tDY2JjTOLq7u7MWvOEiXWw+n49x48ZltT8t1HYmX+0B404gfUYdjV9q8ga3282kSZNyHQYNDQ0cc8wxuQ4jKYMdm7Y+7Ey+Zp291kc49TYjeWRAjWaQ0UJtZ/K2w4sp1GmyZV2ep9FkjBZqO5OvYqa7kGs0g4oWajuTr1Uf2XR4ybe7BY0mB2Qs1EIIpxDiLSHE40MZkMZCNE8b3KRZ9ZFBF/J88981mhyQTUZ9NbBpqALRJCFfs07dhVyjGVQyEmohxDjgHOCuoQ1HE0e+Tm6bjUedbxchjSYHZFpHfSvwbaAk1QZCiCuAKwBqa2vjRqLKBr/fP+D3DiW5iGvqnt2MAQ43N/F2imPb8XzV79rCFCDc050ytkk7tjEB8He08+Ywx2/HcwY6rmyxa1ww+LH1K9RCiHOBQ1LKNUKIpam2k1LeCdwJsHDhQrl0acpN09LQ0MBA3zuU5CSutodhP1SWl6U8ti3P16o1sA28Lkfq2EIrYBcUFxUOe/y2PGfouLLFrnHB4MeWifVxEnC+EGIH8CDwESHE/YMWgSY1+dp7T1d9aDSDSr9CLaW8Xko5Tko5EbgY+JeU8rIhj0wzssvz8rXXpUaTA3QdtZ3J1wY33eFFoxlUshqUSUrZADQMSSSavuTrvILa+tBoBhWdUduZfLU+dEat0QwqWqjtTL5OAKs7vGg0g4oWajuTtx1eMpk4IE8vQhpNDtBCbWd6s848m/KoN6NOMx51vvrvGk0O0EJtZ/J1POpejzqTqbi09aHR9IcWajuTr7XGuupDoxlUtFDbmXzNOrOq+sgzW0ejyQFaqO1MvmadWVV95Nln02hygBZqOzOSrQ9d9aHRZIwWajvTW56XZ/ZAJtaHrqPWaDJGC7WdGcnWR77eLWg0OUALtZ3JVzEz4nWku8Dka+mhRpMDtFDbmXyt+simMRGZf9aORjPMaKG2M/madWYzC7lle41Gkxwt1HYmX0vYssqoyb8LkUYzzGihtjNm1plv1kA2HV5AZ9QaTT9oobYz+Wp9ZJtR59sdg0YzzGihtjN5a32oC0xGgzIlPtZoNH3QQm1n8naGF7MxMYNhTi3bazSa5GihtjP5OrltNl3IQQu1RtMPWqjtTJ53eElrfeiqD40mY7RQ25k4eyCPKj+y6UIOOqPWaPpBC7WdiVo83nzKOs2MGpk6bl31odFkjBZqOxONAEI9zqes0+o/R1M0KOqqD40mY7RQ2xUpVabp9BjP80jM4vznFEItdWOiRpMpWqjtiileplDnU9YZzUCoo3l6EdJocoAWartiip3TrZb5lHVahTeSSqjDlotQHn02jSYHaKG2K2Ymmo9ZZ1wjaCrrIwIOl/FYC7VGkw4t1HbFFOZ8zDozbUzMx4uQRpMDtFDbFVPgXKaY5ZFQS0u1SrrGRGcefjaNJgdoobYrZlbqMD3qPMo6oxFweY3H6TJqV+yxRqNJiRZquyITGhPzScxkBkKdr6WHGk0O0EJtV3qtD0Pw8skeiEbAmUFG7cjDihaNJgf0K9RCCJ8QYrUQYr0QYqMQ4gfDEdiHnmhCY2I+ZZ0ZZ9Tm3YIWao0mHa4MtukBPiKl9Ash3MBLQoinpJSvDXFsH256y/Py0PqIRmNCnayOWsqExsQ8+mwaTQ7oV6illBLwG0/dxl8eDeWWpyT2TMyn0fNkBFw+9ThZRt372bT1odFkQiYZNUIIJ7AGOAr4tZTy9STbXAFcAVBbW0tDQ8OAAvL7/QN+71Ay3HEVdu7mOKCxpY0aYPXrrxIo2pPzuDLh1HAIf6CHUuCttW/Stq0r7nURDbEEONzupxJYt3YNrduDwxafHc8Z6Liyxa5xwRDEJqXM+A8oB1YAs9Ntt2DBAjlQVqxYMeD3DiXDHteBjVLeUCrlQ5er5YGNSTez5fm6sULKu89QcW9t6Pt6MKBeu/+TxjYrhjU8W54zqePKFrvGJeXAYgPelCk0NauqDyllK9AAnDl4lwpNUvp0Ic8je8BaehcN9X09n8cx0WhyQCZVHzVCiHLjcQHwUWDzEMelSayjzpcGN7OCo9ejThJ3nxpxLdQaTToy8ajrgHsNn9oBPCSlfHxow9L0ile+DXNqirDZ9T1ZY2I+lx5qNDkgk6qPt4FjhiEWjZVe68Ps8JInVR+9IpymjtrcRnd40WgyQvdMtCv5an30ZtRmHXUSjzqfu8drNDlAC7VdSWxMzBcxiyYIdbK4tfWh0WSFFmq70kfM8sQekBlYH33uFqLw2h3Q9MHQx6fR5CFaqO1KnxK2PMk6e6s+MvCozc8W7oGnr4MNDw99fBpNHqKF2q70meElT4Q60aNOVked2D0+FFDLcM/QxqbR5ClaqO1KPlkfzVuhfb96nI1HbVZ9hLrVMnKE3cibt+ZPdYxGkwVaqO1K4uh5dhbqv34enr9RPU4sK8zEow4bY4EciVA3b4Xbj4XtKwe+D43Gpmihtiv5ZH10t0J3m3qcSXle4t1CyBDqI7E+WneqZceBge9Do7EpWqjtSi6tj5ad2V0Ywj0QNuyLPtZHBhm1KdTJRD1TAoeNfXUOfB8ajU3RQm1XclX10eOHXy2CN+/J/D3hnpht0dtQmEUdtSnykSPIqE2hDgYGvg+NxqZoobYruerw0tOhBHPny5m/J1lG7XQhEf1MHJBgfRyJRx1oNvalhVoz8rCPUEsJdy5l3O5Hcx2JPUj0qIfL+jAFd++azN8T6YGwmVEbcQsnUjgzG+a016MeBKEO+tNvp9HkIfYRaiGgfR+Fgb6zmHwoydWYzWaDXusu8Df2v30krGIzbYve0jsnUjgyrPoYDOvDFGqdUWtGHvYRaoCS0Xh7Duc6CntgCly6euShwCqW+9b2v70psubSFGGHCylcKTzqBFtnUBoTtfWhGbnYTKjr8AS1UAO5mwDWWiKXif1h+sqmbWF2Ie+1PjIY5rRX7AejMVFbH5qRh82EWmfUvfSZimuYMmqrWO55M4PtU2XUDmV9ZDLM6WA2JmrrQzMCsZlQ1+EJtR3ZLTDAOw/DU9cOTky5Ild11KZQ18yA/esy394UWfMCkzajTrhbCB9hF3IptfWhGdHYS6iLa9XSf+jI9vP+M/D2Q0ceTy5J7EI+XB61KZoVE5Sd0N/YGaZQJ5bnOZypPerEipYjzahDgZi3rq0PzQjEXkJdUqeWR9oNOOiHYJ73UOtTnjfMjYkFlYCMiWh/28uoUQFiLc9LUfURjTU4An3L8165PbvyQDObBm19aEYkNhPq0WrZsf/I9mN22ogkEYl8oY/1MUyjwpkZcmGlWvZ3wbN62pGehIw6RR11nEct+pbnvfBDWPdA5jGbQl1Qoa0PzYjEZkJtZtSDINSQ3+M+JGadw2Z9JAh1f+fQKtThnpiXnknVh3CCcMRbH9GIWva0Zx6zKdTl4/P/TkqjSYK9hLqoGonjyK0PU6jtdht8eDv8/crMeuDJiBJp4Yg9Hw7CVuuD/oUvkiDU0VjVR9ThTOFRG2LucKo/M+sOB2PZdXc2Qm1UCpXVa6HWjEjsJdQOJ0FPxeB41GC/H+2OVbD+ATi8rf9to2GVcTqc6vlwVX1EjsD6CHcn6UKeLqN2qM/Ye+xgbBKBgWbU0dCRVw1pNDbDVkJ916pttDorRq710StCHf1vG40okTaFbLitj0wz6jiPOtjXo05XR+1wxu4YzPXmRdYc3zoTAs1qP6VjMotZo8kzbCXUv3juffZHy48so45GYg1KdvvBhrPIFqO5sj661cXBV6qeDzSjdrgy86gdzvjXzHOTrfVRUAme4sxi1mjyDFsJdbHPRbOoPLKM2pqt2s2jDmeRUcuIEunhtj7CPeDyZS56cR51ME6ElVCnqaNOzKghJtDZZtSFleApUs915YdmhGEroS7yujhEOXQdjtkE2WLt8GC3zg9ZZdRhI6M2rY/hFGovuAvV86yqProzK89LrPqwYgp0T3vmn7m7FXzlMaHWGbVmhGEroS7xujgsjVvu7taB7cSardotsxqQRy3U8+Hs8OLyZi56iXXUMtZQGHW4k19wLWLex/rozaRl5hfaUDe4CywXF5t97xrNEWIroS72uWiL+tSTngFmw9b32S2zytb6cLiUUAvHMFsfAxTqcHxjYsTpSy6aMkXVB8TfbWRqf4QCSqh1Rq0ZodhKqIs8LtojxvjLwQzELBnWH3qmP9iOg/D8D4a+siIboY5GYiKWyusdCsLdar5Dh1N51Vl51PHleVGHN7lQR9N51BZxzrREL5yQUWuh1owwbCXUxT4XrUeaUQcHkFG//zS8dEtm9c1HQtZVH8bXIxzDWPURjE1W4Ck6ovK8iNOXvEFXpqn6sAp1ppUfoS5wJWTUb92fXeWIRmNjXLkOwEqJ18XeiA+cDDwrGohHbR6rvwGIjpRQFr3uzMZEUGI2nHMmDlSow91xXcgjTm/yxkhr9/h0GXXG1kdXvPWx5w1Y8wdAwDGXZrYPjcbG2CqjLvK6aI4YGfVAKzbMTNzhznwf5nZDLdTZWB/te2OdToRz+Ko+IkFleQC4i/qv+oj0xCyaBI866vCpC05ip5ek5XlGo6n1bmOg1oc5jnY2vRs1GhvTr1ALIeqFECuEEJuEEBuFEFcPVTDFPhcd0QL1JBMxS4b5vuLazOuoe4V6iKsFMhXqYCfsXQsTTlTPh9X6GEBGbXaOifOoHSqjhr77sEzX1Wt9mHXb2WbUUqrvzeWLCfXBjcZxbVaeqdEMkEwy6jDwX1LKGcBi4GtCiJlDEUyJ10UnR5hRBzvUj7agPHP7ZLisj0yFevdqVX888RT13DHIVR+v/w5W3pwixqBqTITMhdprCHXCMKcRp/FdJl4ALdN19Wbj3hK1zFaoIyF1btwFan/uwtgEBLpRUTNC6FeopZT7pZRrjccdwCZg7FAEU+R1EeBIy/M6VHbmyeC23aRXqIc4o860jnrHS0rAxh+vng921cfGv8O7jyZ/rU9G3c85CfcY2bBQIm/pQh7LqBP2EVfRYvwLes2Mul2Nwe30ZmZdmN+Z27gTM7NqGPj/kEZjM7JqTBRCTASOAV5P8toVwBUAtbW1NDQ0ZB3MjoNhojgIObwc2LqJrSLzfYw62EBB10EKA3spjbro8vfgjLTyVgZxzNq7nRpg84Z1HGiuTrqN3+9P/ZmkjHVMScNxHS0UAuHOw7yUZF/1u/5OYWAPRZ07oXgya19Vs5ycGArTtHcP7yd5T9q4UrCweR+OaJDVSd632N9Ga1MLmxsamNHip7S9kdfT7H9O437coSBFDjd7t28h7CpkMvDiqpcoCqrJDt58dSX+kj2975m0czv1CFY2NLAw0EUx0N4jKQWCHU04cBN1uGnatjnpZ7bi6TnMicB723ezv6eB46MODMnmwK6tbB6kczYc6Liyw65xweDHlrFQCyGKgUeA/5RS9kl1pJR3AncCLFy4UC5dujTrYDxbm7jtrdeRnhLqR5VTn80+HrgT9q9Svq6jhoLycXB4OxnFsetWaILpk8cz/fjk2zc0NCTfV8dBuHMJnHMLTD87/XHWOqALXJEulp56aqz8zuSe/4UDr6jHJ10dO96aAsbU1TImyfFTxpWONSEQ0eTve1MwetwERi9dCu1/g/c2p9//jpsh6oPQIcaPqVUNoNthydKPsP7RDQAsnDcTxi+OvSf4Auxzq/1uLoVOKK2ug44teKJdaqYWbwljKouSfuY4Dm+HV2HazHlMm78U3q2GbjXn5ujKYvU5EhjQORsGdFzZYde4YPBjy6jqQwjhRon0n6SUfxu0oydQ4lUTuYZdGXijiYQ61a1y42blmXqKsqj6OALr49Xb1SBSZgNWOsKmBy6T2zI9HWqWG18ZTD8vtl44Brfqo7s1taUR7o5VfXiK+z8n4R5wGVZFn8ZEs70h4bPKaKwR0bQ+PIZHbVadeEsztD6Mc+o2K1Us1oduTNSMEDKp+hDA3cAmKeUtQxlMkVf9eIPOwux/ZOYPtmWHxaPOsurDbOzLlM5meOMe9bjrcP/b9/q5JPepe9ph0qlw7U6oXxRbL5yDV/URDqrzEvQnn4cxHIzN02g2Jqa7SER6lEi7fLHyPOEAIVTPROj7PVg9alOwTY8alN/sK82sMdG8+JkC7TGWVUfrxkTNiCGTjPok4LPAR4QQ64y/fu7xB0axTzkxPY5C1RAkpbIWrGz6J3zwfN83WzNEb4n64WZc9THA8rw371aZsbsIulr63z7UBUU16nGyTi897SqTTPS7B7PqwzroUWKVi5QJGXWh2i6cphrGHBvE5Yll1IYIp2xMlNZelwnleaCO7yvLrGOQ+RmsdwHuQqjWQq0ZOfTrUUspX6K3N8LQYlofXaJAldm99xQ89Dn4xkYoqVUbPX09VEyEoz4a/2arleAtjt22R6N9veBEBlqed3AjVB2lhCHQT0YdCSuBKh4FLdv7ZtRSqnVmmZoV4Ri8qg/rqITBzlgGCkbHFKlEFyxjUgdivf4S6RVqX6wLucMU6hTleemqPkBl1N4MM2qzksbMqOuPN8oF5cBr8TUam2Grnok+twMBdAmfyqib3lP1xG271QYdB9XjZN6lVWS9JTEByiRLHqhHHWhWGXJBRf8ZtZmVmhl14mcId6tefEmFehCtD6v4JdpL5gBLvdlpUfLtrJhC7TQz6mivCKe2PkLx3eMheUadiUcdTvCoT/5P+Pjv1P50Rq0ZIdhKqIUQFLggQIESB9P26GxSy31r1TLZLbH19tpTkvlsH5FwzJvONqPubISiajW7SH8etTkmRq9QJ2R75mcye/lZGcyxPrpaY48Tz40Zo2lZZDK+c69H7Y3NQp6YUSdaH10t6uIGsYza5VXd/sHwqMvUcfubqLbX+iiIX59JZx2NJk+wlVADFLgEflmgMmq/MXdiwBDqvaquOHlGHYDi0eqxt0T5xtB/o6T19ayFugkKqzPLqM19F49Sy0ShNp97kwj1UFofVkyh7u3wksF0XObUXS6vsj7MKcQA6XAp8U2scOlsUhc4iAm10xM7rssXE3LzIp2K3qqPRKEuVtl2JMmcjRpNnmE7ofa5oEMao66Zk9yaP9a9lozaWrEQCanb6XEL1XOvJaPur2edVYSysT6iEYv1UamEOl11RJ+MOuFiYz5PaX0MVmNia+xx4kXMvLPI2vqwlOdZMmpAZeWJF8DOxth5MLd1esBpyahrpqnHh/ope0wl1KbnbbeZ6DWaAWA7oS5wCdrNManN8aE7G5Uwmxl1NBQ/vKYptuMXw/Ifw/RzYh51f7e/cUKdRUbd1QJIlRkWVCghTeep9nrURibZJ6M2hTqZ9TGIVR9W6yPx3JhjZPQ2JprnMMUFLBpV30Wv9RGMzUxj4klSfWMVamtG7bRk1KPnqsf716f/POFUGbWe7UUzcrCdUPtcglZzlhe/4VEHmpVod7fC6DlqnVUUTYH1FMOJXzdmpM4wozJnkhHO7DLqzka1ND1qSO9TmxcWd5GKrbsN2vbGXu+1Poaz6iPRo07MqPuxPiIWq8TljTWIisSM2nKccFB99l6hdlr2YVwg3AVqUK3yCbD/7fSfJ7E8z8Q6Gt+jX4UD76Tfj0ZjY+wn1E5oCXviV3Y2wQHjBztpiVpaGxRNIbCWkGU6LZP5elF1dhm1aceYHjWk96mtPei8JbD69/CLWdC8Va1PK9SDXPVhimMf68NsTLR0eEm2XeL2plBHgkY5pEWoPYXxFwSzvcG8s4izPozjmqJbN6//jDrUpbZPrD03hfrQJlj3J9i6Iv1+NBobYzuhLnAJWhOFOtCkehwC1M1Xyx5rmZkhttbb32w96qKaLIXazKhrYgP8pxPqsKX0razeuOWXMXunt+qjrO97zaqPna9kPsZ2KrpaVTd1SNOYmNAde8X/wG9O7JvVW4Xa2oXcOmtLYkZtPW+Q3PpwW4S6ZXv6empz0oBEzO/fvBAO9RC2Gs0QYkOhhqaQRaiLa1VX7ZYdUFgFpWPU+u4k1od1nIdMGsIgIaPOxvowM8OaWEYdSCfU5i26Fy75C1xhZHhmg6mZUVvriU2EA/yN8Iez4Y3fZx5jMrpbobQ/obYMc1o6VmXKhzbGYjWJWMr5rF3I+zQmZiDULktjollqVzdPLdPZFqFA39I8M3aA5i2x7TSaPMV2Qu1zCZqCFqGunaV+3C07VY9Es844zqM2BMdqffSKZz/1zaZAZptRB5oAofzpwmwy6gK1feUU9dz04XvajTI3T9/3CqfR6cfSoDpQutvUHYC7sK9/H0kQaodT9Qq96G71vH1v/PbWDNzlUe+3dCEH+o5p3ZnG+nAlyaghvU8dSpFRmxZS8wfGdlqoNfmL7YS6wCVis7wAjJqpstFD76rGJbMqIm4S2yQt/y6vEqSOfekPaLU+wt39j1LX06HEprNRCa7DCb5y9VpiY+Jdp8PLt8XHaBUjX1m8UCer+ABV9WFemPrzbPujq1U11CUbCyXR+gDl/ZaNU4/NHqJ9tjdsi0jQmJQ3MaO2HMfaCAsW68Nr8aiN77F4lLqjOrgh9ecxJ7ZNxLxoN2mh1uQ/thPqMq/R4QVUmVf1VPXYfzA+o7ZaH2bG5rZk1KBsksTb9USs1gekH4AI4OnrlAXR2agaEgGcLvCWxWfUPX7Ysxr+9d/KtjErKqyiUjzaItQpxvmAeM+3ZUdmA0BJCQ9eCqt+Hr++u1VdWJL13EtsTDTpFeo9Kbb3xi5Aoa6EjDqhMbGzUe3fvCiZ2zrdseO6LReKysmx9olkhLv6VnxATKjNtgztUWvyGNsJ9agCS0ZdXBvryQdKqL1prI/EzKqkDtr7y6j9KoMzx0Pu7wd9eIcag2TPmpjPCipLtdospk0Q6YFnvtPX/wX12ToyEWpn/PNMsup3/gqbH4fNT8TWRaPqAucri42F8fqd8OS31euJ5XkmvlJ1IUoU6sTyPFDnz5HYmGg5p51N6ryZVRoOZ999WD3niknphTqV9ZHo9R9pI6xGk0NsJ9Q1hQ4CGD/Y4lGxrBWgYoL6YbuLkjcmWkeCAygZrQb1T0fQr7Iv88fe3y2yWV7WsQ+KqmLrCyvjM11T1CaeogTTtA2sIlRiyai725OP8wExMTMbS/sT6mAnPHeDety0JdaLs6cdkOqiYk6ssOkxeOch9brVykikbFySjNoUdovIBjuT1FF3xmIwx0cxiav6MHsmWi4UFRPVxdYcJS+RUCC5UDuc8edaWx+aPMZ2Ql3qAZ/bTdBRoKwBqxhWTFRLX2mK8rwk1of/UPqBfYKdCULdT0ZtHXsiLqOuiPeoTVGbcb5aNr4HiJgYgbpj8B+0DHGaQqhNMauaAmXjYd+69DGu+aO6kMz6uBJn0/4xO7v4ymKWRPtedYHp8fcdPc9K2bgkHrXZk9EbK60LBfr2TJTR2EXA2ivR+tnieiZaM+qJgITWXck/a6ryPIhvXNZCrcljbCfUQgjGVxbS6KyFmqnxPdhKDa/UW9q3w4twxosgGPXCUol1KoKdsYkGzH2lIho1xNi4bbdm+wUJGXX7XrWdOVNL8wdKUKwdM4pr1fF6OjKzPkrqYMw82L+u7zaHt8O/fqwyzzfuhvrFcOzn1GtN76ulWY/sKzesD3+sd2Tb7r6j51lJllEnlueBEv+4xsSEUQxN68MkWYeXxIwaUtsfqcrzIH6Ma+1Ra/IY2wk1wPiqQr5e8FNY9h0lKE6vEgqnkan5Svt2IfcU9e2dZnbsSGd/9HQkZNRppuPqalHZ4aRT1HPrLXxBhap1NgWhba+yNiomGc93x/vToIQa1IWkpy191Qeo/dUvVp1kDm+P36bhJ7DyZ3Dfx+DwVlj0b7GBjUyh9hsVF4VV6jO37oqJbesuJdQOd/KJFsrGxTJvk7ieiYbIBjv6NiaCElQpU1sf1n1YhbfSOH8phbo7XtitmD51tsMDaDQ2w55CXVnI5haBdHqU+BZVxzIrMCY+tZTnBTuT3/6aHTvSNSj2Wh8ZZNSBZrWc80mYdwlM+UjstRnnKpF66lrjmHtUZ5GC8pgAJ2Z+ZkNpx/7Mqj6KR6vjgJqSzKS7Hd59TGXKu15Vmf7M89WFylMSE2qzprhqSt/Jf02hTmZ7gOpNCfG11OZ5LR4FtbOV5dHd1rc8D1SmHfQrqyLO+uin6qOoRu3DFOpDm+Cuj8ay+3B3fEcnK6b1UV6vGxM1eY1thborFKHJb3igJ3wNFn0ptoEvifWR7MdaYvRiTFeiF+w05tnLwKM2GxLLxsGFdyjBM5m8FE7+Jqy9FzY9roSkbKyxvSFyiRl1iTF+dssOlan3a32MVhes0XNVI6DJxr+rMrWL/wzTzoFl/08dSwg1d2Dje2q75i2qesMUPyttu435EpM0JELss2x7UdWGS6kuAMWjleddNQVO+LoRb0LVB6gGRVNszXHDwWJ9JKmjBvUZKiaqruQAr/8W9rwBq4x5lkOB1BcXM6OunKKtD01eY1uhBth12MiCTvgazLwgtoE3hfWRSGGVupVP1+kl6I9NiAqw7y347cl9/ViIH4gpGcu+o3z0dX9W1ofpqZt1yIlZv2l9mN2c+6v6MK2cmecrsTL95XV/guppMOFE+Myf4y9qNdNU5QcoYa0+WomftXzNVw6tu1VWXWQph7Rifoanvg3PfU/ty9yfyZJvQ/n4+PFKrEOlvv+0ejzp1NjrvY2J7r49E00qJiqRDwbgnUeUoL91n+qtGg33n1FXTdHWhyavsaVQ1xtCvftwih+XtyShw0sK68PhUFloewqPun2fyiSrpsTev/HvamyJ1UnG1Egc+S0RpwumLocPnlMZrpmFlqfIqAsq1IXE7D3XX9WHmYHPMC5a7z5KQWAv7H4d5l/S16MHJaQd+9T5avogJqymiLl8UDdXZay7XoMJJySPoaROxWE22O5dawj11Ng2niL4t3/Beb+MreudaadT1XSPXRizpMzPZlpc3lKjK3nCd2nWUr/7D2UvnXebugN58WfGMdJk1J5iZc1EQ/1P66XR2BRbCvW4CvVD3dmcQqh9ZcY0S8YPL9SVOqsqqUudUZudQaafGxPqw8Zoa2/d17dhsdPwqAurSMnRZ8QG4C9LyKgTb9GFUFm1WRedSdUHqGqYcYtg9Z3U7X9evT7v4uTvHTVTLXe+os5D1VHquSnUpWNVFrxvnRLBCScl34/TDWf9DC5/XJ3rLc8oP9pssDQpromNfQKq9t3lg5duUXcr08+O337Wx5VVA7DgcvjC033tl4qJKiN+8hrVU3HexTDjPNjwiHo9lfUx/xJ1l5NJ+4NGY2NsKdQ+t5Ox5QW8f7Aj+QZm5tm+V41dEepMLdSldZYR6vywfVWscmHzE0q4aqbFv79kjGo4fPfR+H0FmtSxEzNjK5NOjZW39VofZkadRFAqJqqGR09JTEQTEQ71Z22EO/EqaNnBuD3/gKM+Gsu2E5l4sspSX7ldPTczYFOoy8aq2mxkbPtUHPdlGH+8Gixp85PG/o5OvT2obPa0G1QjJ6iLopVxC+Dkb6jHvjL1PJEZ56pSwxnnwdk3qQvcjPMts7uk+O4nnQInfDV2EdYNipo8xdX/Jrlh8eQqXth8kEhU4nQk3NKbXu5dpyuRDQb69ko0KR0LW55Tw2++9AtYdbOyG+ZdDDtWKf9bCEN8BSCVx7v+QXjhh2p6L7PipLMpPltMhqdQCcQHz1usj/FqmUyoP3GP6vQyakbfOnCTwkqVSTotX9f0c6FiIo6WHSpzTIW3RDV0bnlWPU+0PsrqY/FVTkkt+FbGHBMT3upp6bcFOP5KeP8p1cXeapVkSukYOP/2+HVHn24MBNWT2vowSazl1mjyDFtm1ACnHF1NayDExn1JBo03M+rOQ7DnTdUgmCqrGrtA/UD3rYOt/4JRs1R2tu5PqiFq+nlqOyFi+xh7rBLQYCf88byY5RFoSt2QaGXhF1XXcbNhrrcxMYmglNQqjziVSAMsvQ6+8FT8OocTPvI92kumwrSz0sdjZrHCoQQfEqwPI+OfmML2SGTMsWrpLoqND54OhwMufRi++ExyH30gmBcgSP3dm2Ta61SjsSm2FeqTjlKCuGpLU98Xzcao+uPV7a//YOofq/ljfvdR1aNvxnlw7i3wxWdh+X8rITcxf9B185V4XvpXaNsFGx5W6zubUzckWpl+Dnz+8VjHkeLRKotP5aX2h6cofnAqkzmfYO2Cm9JbMWAIuVDDxPZOCmBUfZSNVVmxqwCmnZ1yF3GMOUYtzQqSTHB543sKDgYzjItsv0KtPWpNfmNb66OmxMv00SW8tKWJry1L8G7HHAv/sU51Evmd0UswlfVRVK0mxF39e6NXoVEaVr8o1r3bxF2obADT3qg/Dmqmq84lE6epjNoczD4bHA6Y/XFVPpcLikfB0cvjG0GrjlJ3F+NPVA2A125PPWZGIpWTVcWK2VCZK+Z8UjVojl+cfjuPFmpNfmNboQZlf9z7yk78PWGKvZZQhVBdi8NBlalGQ30HZLIyeZkquXMVwLiFqbcrrYvNvGIy4zxY9XPcY75sjFORpuIjHR+/c2DvGyw+82B89ltYCV99JfY8U5EGdeH53D9S11wPF26fmnW+3+10Y6Imv7Gt9QFwztwxBCNR/vjy9uQbuDyx8rB0QmPaH+OPT28TXPIQnHNz/LoZ54GMUnvwRXVByMSjtiMOx+D5w6DuLKz10HZGWx+aPMfWQj2/vpzlM2v57YvbONwZTL7R6Dlqmcr6AGU5FFb3LQ1LxBynOW7/c6F8PBN2GmM2Z+JRa+xFr1DrxkRNfmJroQb49pnTCATD/PSpTUhz8HkrtbPVMl2DkrsAvrlJjSiXLULAeb/EXzxJNQoOxKPW5BadUWvyHFt71ABHjSrhyiVT+E3DVo4aVcwVpyZ4yL0ZdT8VBakGG8qEKR9h/XwHS5cuHfg+NLkj09l7NBqbYnuhBrhm+TR2Hg7wP09uZmdzgGvPmk6pz6g7nniKGlviqI/mNkiNfdF11Jo8p1+hFkLcA5wLHJJSzh76kPricAh+/sl5jC718YeXt/OPdfu4YP4YLjl+PLPGlMGCz+ciLE2+YHZmSpx1XaPJEzLJqP8I/Ar4v6ENJT0+t5PvnTuTC48Zyz0vb+fhNXv40+u7mFdfzsWL6jl+UiUTqor6djfXaKDvbOgaTR7Rr1BLKVcKISYOQywZMXtsGbd8aj43nDuLv721hz+/vovr//YOABWFbs6cPZrjJlUyvrIIl0MwbXQJPrezn71qRjzuQu1Ra/IWkbSSInEjJdSPp7M+hBBXAFcA1NbWLnjwwQcHFJDf76e4OPOuxlJKdnVE2dUeZWNzhHWHInRHYq97nDC13EmFT+BxQiQKHSHJ+BIH82qctPRISjyCiaUOIlINy+R2qEl2jySu4cKucYG9Ylu0+mt0Fo3n3VnX2iouKzqu7LBrXDCw2JYtW7ZGSpm0R96gCbWVhQsXyjfffDOrIE0aGhqOqLoiHImyramTva1d9IQivPxBM2/tbuFQew/BSBSnEJQWuNnelNqvdDsFJT43xV4XxV4XJT4Xne1tVFdXMrGqiBl1JUyqLmZncyc7mjvxd4eZWF3E0aNKEAJKfW7GVhRQWeThcGeQbY1+5teX43I6CEei7GnpYluTn4PtPdSWeplZV8bostTjgEgpkVJ59YN9voYSW8V251I1TOylf7VXXBZ0XNlh17hgYLEJIVIKdV5UfWSDy+lgam0JU2vVIPxnzk7ee25Xc4C3drcwrqKQA23dvHewgwK3k6iU+HvCdHSH8HeHjcdhAmFJk7+H1dsPEwjGUnanQ1DgduLvCfc5xqTqIva0BAhFJDUlXkp9LnYdVs8TOXpUMRVFHrwuB16XA4/LQVcwwp6WLva0dOFyCk6fUYsEWgNBiowLSMuhHtYG3+Ot3a1s2t9BdbGHUaU+aku8jCr1Eo5ImjuDFHmclBa4KfW5KS1QX3tLIERrIERrIEhrIERLIIjP7eS4SZWMqyigyOMiHI0SjEhC4SihSBSf24nb6aDJ38OoEi+zxpTRE46wp7WLfa1deJwOxpYXMKmmiHWHwux6dQehiKSswE2Rx4kEolLSEgixs6mTqaNLWDRRja3y3oEO9rV2UVvqY3SZj8oiD+FIlJ5wlJ5whJ5QlFBUEugJs/lAByU+F6dOraHU5+ZgezdbG/0UuJ1UFHmoLPIwrqIAn8tJW1eIcncBQnch1+QpI06oM2V8VSHjq2KdZM4hfXdodYU8hWhUsrslwLbGTuorC5hcXYzDIdjf1tU7I01bV4htjZ2s2XmYj0wfxdxxZTyz8QCRqOT0maOZXFPElJoiakt9HGzv5o0dLbyx/TCdQXVhaPZHCUaieJwOJtcUsWRqDYcDQVZsPkShx0VFkZudzQE6esK0BcI8veMDJlcXsXRaDW1dIQ61d/P+gQ4a/T04HYKqIg9doQjtXSGiCdcIt1NQXuihvMBNRaGH/W1d3PTMe4N3otduTPmS2ymSXrQyQQg1vy5PbEq7ncshCEclr9Y7qHNrodbkJ5mU5z0ALAWqhRB7gBuklHcPdWB2xeEQTKgqYkJVfFfzurIC6soSxxuJdc65YP7YpPsbV1HIggmVXLlkStLX+6OhoYFTTl2StNolEpU4RMxvl8bdQluXmsKsotBDocfZx49v6wrR2NFNIBjB7XTgdgrcTgcup4OeUIRgJEplkYc9LV1sOdhBgcdFXZmP+opCguEo25s72d7op/vgNj6x/GTcDgdtXSH8PWGcDqFmIPO6GF3q49397Ww+0IEAJlYXMaGqkCZ/D/vbumnpDOJxOfC6nHhcDjxOBx6XwOtyMqWmmCZ/D69vP0woEqWi0M1Ro0oIRaK0dAZp6gyyq7mTQDDC39buZW+noK5IV31o8pNMqj4+MxyBaAZOqpLExPVCKO+9xJdmkgKgrMBNWUH6bQBGlfg4dnxFn/XjqwpZMrWGhoadVBerQbDKCpPvb/bYMmaPLYtbV13sZfroFBP9WqivLOydCDkdbV0h9q6DBZ5OdPFmCkLdaiKNwR4zXDMo2H6sD43mSDnl6Br8EQ/hbt3hJSVPfBP+74JcR6FJgRZqzYjnhClVdOJD9HTEJjb+sNNxMPY4GlETPR94B6LR3MWkSYkWas2Ip6zAzbaa03DJILxyW67DyY7dq+Hl26Btz+Dtc/sq+PlUtW9QU9R1t6qJgv0H071TkyO0UGs+FNTNOpXHI4uRL9+Gp6c51+FkTsNP4Lnvwa1z4N3HMn/fm3+AHS8lf23tvWq5faVabl0Re61lx4DC1AwtWqg1HwrOnD2an4YvJhqNMH/dd+Fg6rLBIWfT4/C/E+G+C2Hbi+m3bdoCU05T464/+S01T2gqwkFVsxjsVNv+7SuqkdBKd5uaAxRgj9EpbesKNQcmQOvOAX0kzdCihVrzoWBqbTGe6kn8uPLHOCNdcNfpsO+t4Q+kq1U13HlLYd86lTGnoscPbbth/Alw7q3Klnj+xuQ+8sF34ZYZsPIm2PWamjaufQ+s+UP8dhsfhXC3mth4zxvqGLtfh7kXq9dbtFDbES3Umg8FQgjOnl3HvfvG8eKcm9Xkvn/+NLTuGt5AXvgBdDbCp/4P5n1GXSzCKaaZa/5ALWumwrgFsOhL8MZd8Pul0Lw1tl3rbrj/4xBogjV/hG0N4HBB/WJ1IfjLZ+HRr8LDX4RnvgNVR6t9BZpg1c+VqM88H0rqdEZtU7RQaz40nDVnNJGoZEVzGVz6V2UL/OmTKssFZRsc2qREcCiqHzqbYe19sOALMGY+1B+nstsD70A0wpi9T8DvlsREuGmLWlZPNT7ATXDhncpHfvSrsRif+56yRE75L2jfq8R63CI1ocbouWo/216Ena+oyZo/9X/q2AAv36qmlxt/AlRM1B61TdFCrfnQMLOulBOnVPHI+0Hei46DT9+nRPEvl0FnE/zj6/CbxXD7sfDgZ4w+6sCBDfCHc2D/27Gdvf0QPPtdiISSH2zNvXB4W/y6DY+o7HXRl9Tz+uPVcvfr8NDnmLrlTlWB8eqv1Pqm90A4oXKyeu5wwLxPw/Ifw+7XYP0D0Pi+sjOOuwJO/ia4CqCnHSYtgVHT4fOPw9deg29uhP/aDBfeAbUzoWYGuItARuHE/1B98ssnaOvDpmih1nxoEEJw66fn43MJ/v1Pa2getRgu+DXsehVumQnr7ofFX1XC9f7Tqjoi2AkPfwF2vgR//bzKXIMBeOpaeOV2tS7cE3+ggxvhn/8BT347fv36P6s5PmtnqeeldVA2XvnImx9nx4RPwfxLYf2DKstvel9luS5v/H7mX6pE/slr1PFdPjjha6pX4bSz1DaTTk1/MpwuqF+kjj/T6OhSMUFl5C//Eh7+UtbnVzN0aKHWfKgYVerja/O97Gvt4tK7Xqd5ysfgKythwomw5Do443/goz9QGelT18JvTlDWwbLvQst2ZTms/zN0HYb5l8Hmx+Hu5fGe8XpjLPYPnoP969XjQ5uVHz3vkviA6o9TglxQwe76C1VmHArEsuWaaX0/hMMBn7gHpp4BhzbCcV+Gomr12olXweyLlPXRHx/7LXzhCXAa3fsrJgISnrsBNjwM/kNZnFnNUKKFWvOhY1qlk7svX8T2pk4u/M0rbJb18LlHYdn1ygJwOOBjd8Csjyt/+LxfwpJvwek/gk2PqUy5bh5c8Cv49P3K1737dGjfp3r5vfNXmHCyquxYdYs66OrfgcMNcz4ZH4zpFS/+KhFXoeFdHw8rfqIaE6uPTv4hysbBJ/8I39wMp90QWz/2WCXiLk//J6K0DsrHx56XT1BLhzEj0s6X+9+HZljQQq35UHLSUdU8cMViukMRPv6bV3h6w/74DcrGKj/3sodhweVq3Ylfh9N/CDICJ12tRH3GefClZ9V8jI/8m7JLOvarLPe4K+DdR1Wlxtr74NjPQXFN/HFmXQiL/g2O/0ps3UV3Q8lo5WdXJ8morZTWKRtjMKiZrnzrs29Wyx1aqO2CFmrNh5Zjx1fwz6tOZmptCVfev5YbH9vYOwRsSk66Gr61VdkLJjXT4Jyfqwz08W9AWT1MPRNOvUZ50k/8lxL1U77Zd3/Fo9R7fZYRBMvr4YtPKwtm1scG5bNmRFEVXLcTFn4Bxi9O3bNRM+x8aCcO0GgAakt9PHjFYv7nyU3c++oOHlu/jy+eNJHPLp6YcmjWXj/YyvxLVO8+lxfGLgC3MbXap+6Du05TNdNl4zIPrLASTv7PrD/PEWP61RNPghd+qEoKNTlHC7XmQ4/P7eSHF8zmUwvrufnZ97j52fe5o2Erly6ewJdOnkRtaer5LOMwKy6sVE6Cb2xUlRn5xIST1fLVX+EQi3Mbi0YLtUZjMntsGX/8wnG8u6+d363cyl2rtvHHl3ewfFYtp80YxZKpo6gsyqCRLhF34sw/ecDYY2HyUnjpFk5yeGDbXDVOSNdhqJyiektWTFIXoMZNcHiHqkIZNRMiwVhZYccB8B+AolEw4SQINMO+taoBtrAKCsrVfmumq9puRxI3VkpV5y6javvEckUrwU412NT4E9S2nc3qTifZfgcDs9Y+YZakwUYLtUaTwMwxpfzy4mP4r9OncddL23jynf08/vZ+HAKOGV/BR6aP4iPTRzF9dEmfacxGDE43fPZR2PUq+5/9DePcraqBs7BKdeR5/xnVFR7AV65snee+l36fhdWq23rKY3pU13fhVFUrvjI160zgMASNccSFU/n+ZeOY2dwCLmO8lgMblFhua1BxFVYr8d/5kqpmmXiKush0NqrKnMpJqmSy6X1lZY1bBEcvV20G+9+Gg++ohtzmD9TgVXMugpkfUzE2va8qfDr2wTuPqMbci+6GqqPUcLR1c4/o1CdDC7VGk4LxVYX88ILZ3HjeLN7Z28YLmw+xYvMhbnrmPW565j1KvC5Gl6kZ02tLfYw2Zk+vKvLgdjoYU17AuEqVTTd29NDeFWJqbQlFXvWzaw2oOSELPTb9GQoBE07kg6ODjFu6tO/roS7VBd5bqkr6mreqzNfhUjXnkZAS95LRSkjffwpGzVC2SuVklV33tIOnGA68DY2blYjKqOpE1N2qShoLKlSG7nQpgdy9Glp2UNLRBM8blSll41W8o+eq9oI3/6A675xyjRp86oPnoKgm1r6w5w21z6OuVDFv/ZcqvVQfXJUtbvqnugjVzYOXboWXfpFwfhxqZMPmLao8E9Sdw7e2DPIXoYVao+kXh0Mwr76cefXlfPP0qRxq72bFe4fYuK+dA23dHGzv5v2DHTR29PSZ5T0RIdREBlLSW2EytryAyiIPAX8Xt258mdpSLyU+N5GoZFSpl0K3i4Md3bQbEwQHeiJUFnmoryxgXEUh4agk0BPG63bg7w7j74lQWeTG43LgEIJRpT7cDqEmK3YpCyAUVjPdF3qcjKsoIBCMsLelix3NAaqLPZw4pZqZY/qZt9JdEG/rVE1Rf6AGkbIyagbMTaghL6mNPR49O/2xkvB6QwNLF80GhKpYsTLnE9ntLBpRdwpdrUqkS2pVD1SnW/0d3qZGKAx3q89YPgG8Jeq1rhZ47Q71fNTMmB0yiGih1miyZFSpj08vGt9nfTgSpckfpCUQJBiOsrslwP5WNR50VbGHIq+Lzfs7ONzZQ0RKJlQW0R2KsLXRT2tXiGg3lPhcbG3sJNATxuEQHGzvJhSRVBZ5KC9wU+xzUeB2suVQByveO0RPOH7wKCGgwO0kEIwM6LM5HYJIVOJxOvjbV0/sM/Gw7UhWgTMQHM6+nYs8lomTKyfHxlxJpKAClv2/wYkjBVqoNZpBwuV09FohAPPqy/tsc8as0Snf39DQwNKlx8eti0SlEk5X38awaFTS3BnE43RQ6HXSE45S4HbidAi6QxHCUUkkIjnQ3k1USgo9TkIRJewepxOPy0F7d4i9LV0UeV2MLvUxrqKAA+3dXHTHK/z7n9bw+NdPOYIzohkstFBrNDbG6RA4HckbLB0OQU1JrALC7YyJuc/t7H2csh4cGF3mY2ptSdy6MeUF/OqSY7n4zle5+Pev8ZVpesLbXKN7Jmo0mj4smFDBXZcvYmdzJ995uYurHniLh97czc7mToJhLdzDjc6oNRpNUpZMreHhK0/kvx9+lde2NfPP9ft6X6sq8lBb6qO21Ntb9WJWvpjrK4s8I7d8cZjRQq3RaFIyc0wpV8z1smTJEt472MH63a0caOvhYEc3B9u6OdDezTt722jy951OzON0UFOihLy62EOpz01ZgZvSAjelPhdlhep5WYGHikI35YUeygrcKa2eDzNaqDUaTb8IIZg+upTpo5OX7AXDURr9PRxo6+ZQuxLwA+0xMd/RFKCtK0R7d6jfipRSn4sKo8ql0OPC63bgdTnwupz43I5eQW/cEyLwzn48TgdRKSkrcFNV7KWi0I3L4cDrdsR59fmMFmqNRnPEeFwOxpYXMLa8/+7yoUiUju4wbV0hWgNBWrtCtAVCtASCtAZi61oCIbqDEQ53BukJRekJRwgEI7R1hXrLEv+4cW3aY1UUGmLvcuBxKcF3O1V9eYnPRWWRh8oiVTpZ4FYXAp/bic/tNJ4nrPM48bkcxtKJY5iyfy3UGo1mWHE7Hb0CCUUD2kd3KMKTL7zI9LkLiRi9jNq6QjR39tDSGSQqobMnzMGObgLBCMFwlGA4Sk84SigSJRKV7G/r5t397TR3BgfcQOpxOnqFvMDjpLbEx0NXnjCgfaVDC7VGo8k7fG4nlT5H/70nMyQcidIdjtIVjNAdMv+idBmPzWVPn3XR3u27QhEKhshq0UKt0Wg+9LicDoqdDoq99pREXUet0Wg0NkcLtUaj0dicjIRaCHGmEOI9IcQHQojrhjoojUaj0cToV6iFEE7g18BZwEzgM0KImUMdmEaj0WgUmWTUxwEfSCm3SSmDwIPABUMblkaj0WhMhOxnkGshxCeAM6WU/2Y8/yxwvJTy6wnbXQFcAVBbW7vgwQcfHFBAfr+f4uLiAb13KNFxZY9dY9NxZYeOK3sGEtuyZcvWSCkXJn1RSpn2D/gkcJfl+WeB29O9Z8GCBXKgrFixYsDvHUp0XNlj19h0XNmh48qegcQGvClTaGom1sceoN7yfBywL8W2Go1GoxlkMrE+XMD7wGnAXuAN4BIp5cY072kEdg4wpmogzVTFOUPHlT12jU3HlR06ruwZSGwTpJQ1yV7otxuOlDIshPg68AzgBO5JJ9LGe5IeLBOEEG/KVD5NDtFxZY9dY9NxZYeOK3sGO7aM+ktKKZ8Enhysg2o0Go0mc3TPRI1Go7E5dhTqO3MdQAp0XNlj19h0XNmh48qeQY2t38ZEjUaj0eQWO2bUGo1Go7GghVqj0Whsjm2E2i4j9Akh6oUQK4QQm4QQG4UQVxvrbxRC7BVCrDP+zs5RfDuEEO8YMbxprKsUQjwnhNhiLCuGOaZplvOyTgjRLoT4z1ycMyHEPUKIQ0KIDZZ1Kc+PEOJ643/uPSHEGTmI7SYhxGYhxNtCiL8LIcqN9ROFEF2Wc/fbYY4r5Xc3XOcsRVx/scS0Qwixzlg/nOcrlUYM3f9Zqi6Lw/mHqs/eCkwGPMB6YGaOYqkDjjUel6A6+8wEbgSuscG52gFUJ6z7GXCd8fg64H9z/F0eACbk4pwBpwLHAhv6Oz/G97oe8AKTjP9B5zDHthxwGY//1xLbROt2OThnSb+74TxnyeJKeP3nwPdzcL5SacSQ/Z/ZJaO2zQh9Usr9Usq1xuMOYBMwNhexZMEFwL3G43uBj+UuFE4DtkopB9oz9YiQUq4EDiesTnV+LgAelFL2SCm3Ax+g/heHLTYp5bNSyrDx9DXUEA3DSopzlophO2fp4hJCCOBTwANDcex0pNGIIfs/s4tQjwV2W57vwQbiKISYCBwDvG6s+rpxi3rPcNsLFiTwrBBijTFiIUCtlHI/qH8iYFSOYgO4mPgfjx3OWarzY7f/uy8CT1meTxJCvCWEeFEIcUoO4kn23dnlnJ0CHJRSbrGsG/bzlaARQ/Z/ZhehFknW5bRuUAhRDDwC/KeUsh24A5gCzAf2o267csFJUspjURM5fE0IcWqO4uiDEMIDnA/81Vhll3OWCtv83wkhvgOEgT8Zq/YD46WUxwDfBP4shBicKbczI9V3Z5dz9hniE4JhP19JNCLlpknWZXXO7CLUthqhTwjhRn0Bf5JS/g1ASnlQShmRUkaB3zOEt8jpkFLuM5aHgL8bcRwUQtQZsdcBh3IRG+risVZKedCI0RbnjNTnxxb/d0KIy4FzgUulYWoat8nNxuM1KF9z6nDFlOa7y/k5E2qguI8DfzHXDff5SqYRDOH/mV2E+g3gaCHEJCMruxh4LBeBGN7X3cAmKeUtlvV1ls0uBDYkvncYYisSQpSYj1ENURtQ5+pyY7PLgX8Md2wGcVmOHc6ZQarz8xhwsRDCK4SYBBwNrB7OwIQQZwLXAudLKQOW9TVCTYOHEGKyEdu2YYwr1XeX83MGfBTYLKXcY64YzvOVSiMYyv+z4WglzbAl9WxU6+lW4Ds5jONk1G3J28A64+9s4D7gHWP9Y0BdDmKbjGo9Xg9sNM8TUAW8AGwxlpU5iK0QaAbKLOuG/ZyhLhT7gRAqk/lSuvMDfMf4n3sPOCsHsX2A8i/N/7XfGtteZHzH64G1wHnDHFfK7264zlmyuIz1fwSuTNh2OM9XKo0Ysv8z3YVco9FobI5drA+NRqPRpEALtUaj0dgcLdQajUZjc7RQazQajc3RQq3RaDQ2Rwu1RqPR2Bwt1BqNRmNz/j9rGiPyPt/l4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rn_baseline_model/assets\n"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"test loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "rn_model.save(\"rn_baseline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 99.08%\n",
      "Test accuracy: 91.46%\n"
     ]
    }
   ],
   "source": [
    "_, train_acc = rn_model.evaluate(train_ds, verbose=0)\n",
    "_, test_acc = rn_model.evaluate(test_ds, verbose=0)\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOilU58o+F3Cx1887JA7Q+B",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "mount_file_id": "https://github.com/sayakpaul/Generalized-ODIN-TF/blob/main/Generalized_ODIN.ipynb",
   "name": "Generalized_ODIN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
