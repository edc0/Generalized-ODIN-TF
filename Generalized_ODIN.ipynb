{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayakpaul/Generalized-ODIN-TF/blob/main/Generalized_ODIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFwWgoioRXy9"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCQ1S81KIbaN",
    "outputId": "5a9b4b37-56fd-4665-b754-a0e267a094d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User name: sayakpaul\n",
      "Password: ··········\n",
      "Repo Address: sayakpaul/Generalized-ODIN-TF\n",
      "Branch name: main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "from getpass import getpass\n",
    "\n",
    "user = input('User name: ')\n",
    "password = getpass('Password: ')\n",
    "password = urllib.parse.quote(password)\n",
    "repo_address = input('Repo Address: ')\n",
    "branch_name = input('Branch name: ')\n",
    "\n",
    "cmd_string = 'git clone https://{}:{}@github.com/{}.git -b {}'.format(\n",
    "    user, password, repo_address, branch_name\n",
    ")\n",
    "\n",
    "os.system(cmd_string)\n",
    "cmd_string, password = \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QtwAr7vgIpS5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"Generalized-ODIN-TF\")\n",
    "\n",
    "import scripts resnet20_odin, resnet20\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWpj-WTpRZvS"
   },
   "source": [
    "## Load CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6mPJL_5JDsO",
    "outputId": "500c0fc7-dd46-45ae-9245-c2ae4ef6111a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 50000\n",
      "Total test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "print(f\"Total training examples: {len(x_train)}\")\n",
    "print(f\"Total test examples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMIkjg6_RbYc"
   },
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5A1ZdFW6J4_R"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "START_LR = 0.1\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtQQLiGNRdvx"
   },
   "source": [
    "## Prepare data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GqNtOj8yKO_X"
   },
   "outputs": [],
   "source": [
    "# Augmentation pipeline\n",
    "simple_aug = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
    "        layers.experimental.preprocessing.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now, map the augmentation pipeline to our training dataset\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(lambda x, y: (simple_aug(x), y), num_parallel_calls=AUTO)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "# Test dataset\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JOCwKSQRfvl"
   },
   "source": [
    "## Utility function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ssNNrdG7K2bw"
   },
   "outputs": [],
   "source": [
    "def get_rn_model(arch, num_classes=10):\n",
    "    n = 2\n",
    "    depth = n * 9 + 2\n",
    "    n_blocks = ((depth - 2) // 9) - 1\n",
    "\n",
    "    # The input tensor\n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    x = layers.experimental.preprocessing.Rescaling(scale=1.0 / 127.5, offset=-1)(\n",
    "        inputs\n",
    "    )\n",
    "\n",
    "    # The Stem Convolution Group\n",
    "    x = arch.stem(x)\n",
    "\n",
    "    # The learner\n",
    "    x = arch.learner(x, n_blocks)\n",
    "\n",
    "    # The Classifier for 10 classes\n",
    "    outputs = arch.classifier(x, num_classes)\n",
    "\n",
    "    # Instantiate the Model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: initial_model/assets\n"
     ]
    }
   ],
   "source": [
    "# First serialize an initial ResNet20 model for reproducibility\n",
    "initial_model = get_rn_model(resnet20)\n",
    "initial_model.save(\"initial_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set the initial model weights of our ODIN model\n",
    "odin_rn_model = get_rn_model(resnet20_odin)\n",
    "\n",
    "for rn20_layer, rn20_odin_layer in zip(initial_model.layers[:-2], \n",
    "                                       odin_rn_model.layers[:-7]):\n",
    "    rn20_odin_layer.set_weights(rn20_layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXUFugU3Riph"
   },
   "source": [
    "## Define LR schedule, optimizer, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8a6VIseo7GFP"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    if epoch < int(EPOCHS * 0.5) - 1:\n",
    "        return START_LR\n",
    "    elif epoch < int(EPOCHS*0.75) -1:\n",
    "        return float(START_LR * 0.1)\n",
    "    else:\n",
    "        return float(START_LR * 0.01)\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr_schedule(epoch), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iUhtb9ZiM17B"
   },
   "outputs": [],
   "source": [
    "# Optimizer and loss function.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=START_LR, momentum=0.9)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkU9afGCRnaC"
   },
   "source": [
    "## Model training with ResNet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3zhJ1iFPLzp",
    "outputId": "a858aef4-4c17-4b90-cc10-413707d2cbff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 25s 24ms/step - loss: 3.2212 - accuracy: 0.2156 - val_loss: 2.6627 - val_accuracy: 0.2122\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 2.1440 - accuracy: 0.3843 - val_loss: 2.1156 - val_accuracy: 0.3638\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.6905 - accuracy: 0.4946 - val_loss: 1.9717 - val_accuracy: 0.4248\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.4642 - accuracy: 0.5604 - val_loss: 1.4531 - val_accuracy: 0.5693\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.3773 - accuracy: 0.5930 - val_loss: 2.0856 - val_accuracy: 0.4416\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.3183 - accuracy: 0.6210 - val_loss: 1.3176 - val_accuracy: 0.6121\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 1.2880 - accuracy: 0.6327 - val_loss: 1.6983 - val_accuracy: 0.5032\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.2360 - accuracy: 0.6597 - val_loss: 1.7545 - val_accuracy: 0.5313\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.2368 - accuracy: 0.6649 - val_loss: 1.5768 - val_accuracy: 0.5787\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 1.2260 - accuracy: 0.6724 - val_loss: 1.9772 - val_accuracy: 0.5244\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.2171 - accuracy: 0.6844 - val_loss: 1.2422 - val_accuracy: 0.6669\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.2058 - accuracy: 0.6871 - val_loss: 1.3608 - val_accuracy: 0.6344\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.2063 - accuracy: 0.6901 - val_loss: 1.6145 - val_accuracy: 0.5705\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.2045 - accuracy: 0.6940 - val_loss: 1.6686 - val_accuracy: 0.5813\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1866 - accuracy: 0.7031 - val_loss: 2.9686 - val_accuracy: 0.3824\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1827 - accuracy: 0.7021 - val_loss: 1.5360 - val_accuracy: 0.5954\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1801 - accuracy: 0.7109 - val_loss: 2.0602 - val_accuracy: 0.5582\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1877 - accuracy: 0.7108 - val_loss: 1.2705 - val_accuracy: 0.6929\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1785 - accuracy: 0.7109 - val_loss: 2.1231 - val_accuracy: 0.5059\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1693 - accuracy: 0.7190 - val_loss: 1.8093 - val_accuracy: 0.5756\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1731 - accuracy: 0.7172 - val_loss: 1.4761 - val_accuracy: 0.6298\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1695 - accuracy: 0.7234 - val_loss: 1.5994 - val_accuracy: 0.5696\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1734 - accuracy: 0.7192 - val_loss: 1.6312 - val_accuracy: 0.5950\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1658 - accuracy: 0.7232 - val_loss: 1.3137 - val_accuracy: 0.6739\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1686 - accuracy: 0.7256 - val_loss: 2.6942 - val_accuracy: 0.4747\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1790 - accuracy: 0.7249 - val_loss: 2.1407 - val_accuracy: 0.5369\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1625 - accuracy: 0.7266 - val_loss: 1.7680 - val_accuracy: 0.5944\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1594 - accuracy: 0.7312 - val_loss: 3.5231 - val_accuracy: 0.3760\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1680 - accuracy: 0.7253 - val_loss: 1.5037 - val_accuracy: 0.6306\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1569 - accuracy: 0.7332 - val_loss: 2.1237 - val_accuracy: 0.4947\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 1.1721 - accuracy: 0.7328 - val_loss: 1.8936 - val_accuracy: 0.5487\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1599 - accuracy: 0.7335 - val_loss: 1.7720 - val_accuracy: 0.5679\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1740 - accuracy: 0.7320 - val_loss: 2.1639 - val_accuracy: 0.4698\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1632 - accuracy: 0.7360 - val_loss: 1.8982 - val_accuracy: 0.5375\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1532 - accuracy: 0.7366 - val_loss: 1.4605 - val_accuracy: 0.6413\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1573 - accuracy: 0.7355 - val_loss: 1.4153 - val_accuracy: 0.6513\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1598 - accuracy: 0.7355 - val_loss: 1.5664 - val_accuracy: 0.6289\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1578 - accuracy: 0.7357 - val_loss: 2.3072 - val_accuracy: 0.4899\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1656 - accuracy: 0.7360 - val_loss: 2.4118 - val_accuracy: 0.5074\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1650 - accuracy: 0.7384 - val_loss: 3.2300 - val_accuracy: 0.3583\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.2077 - accuracy: 0.7229 - val_loss: 1.4789 - val_accuracy: 0.6319\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1785 - accuracy: 0.7333 - val_loss: 3.3345 - val_accuracy: 0.3676\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1496 - accuracy: 0.7430 - val_loss: 2.2601 - val_accuracy: 0.4606\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1540 - accuracy: 0.7406 - val_loss: 2.5165 - val_accuracy: 0.4504\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1918 - accuracy: 0.7288 - val_loss: 1.7465 - val_accuracy: 0.5940\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1720 - accuracy: 0.7373 - val_loss: 4.3434 - val_accuracy: 0.3148\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1916 - accuracy: 0.7296 - val_loss: 1.4290 - val_accuracy: 0.6525\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1589 - accuracy: 0.7399 - val_loss: 2.0232 - val_accuracy: 0.5439\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1577 - accuracy: 0.7402 - val_loss: 1.6583 - val_accuracy: 0.5878\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1510 - accuracy: 0.7437 - val_loss: 1.4947 - val_accuracy: 0.6421\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1608 - accuracy: 0.7403 - val_loss: 1.6471 - val_accuracy: 0.6054\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1635 - accuracy: 0.7420 - val_loss: 1.6645 - val_accuracy: 0.5725\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1686 - accuracy: 0.7412 - val_loss: 3.7614 - val_accuracy: 0.3997\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1536 - accuracy: 0.7478 - val_loss: 2.1948 - val_accuracy: 0.4602\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1529 - accuracy: 0.7479 - val_loss: 2.4754 - val_accuracy: 0.4575\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1719 - accuracy: 0.7412 - val_loss: 1.3206 - val_accuracy: 0.6731\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1512 - accuracy: 0.7482 - val_loss: 2.0947 - val_accuracy: 0.4991\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1583 - accuracy: 0.7481 - val_loss: 1.5528 - val_accuracy: 0.6213\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1505 - accuracy: 0.7483 - val_loss: 1.3974 - val_accuracy: 0.6741\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1702 - accuracy: 0.7439 - val_loss: 1.3828 - val_accuracy: 0.6668\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1528 - accuracy: 0.7503 - val_loss: 1.5208 - val_accuracy: 0.6342\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1611 - accuracy: 0.7454 - val_loss: 3.5353 - val_accuracy: 0.4606\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1516 - accuracy: 0.7468 - val_loss: 1.5374 - val_accuracy: 0.6336\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1707 - accuracy: 0.7412 - val_loss: 1.9995 - val_accuracy: 0.5551\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1484 - accuracy: 0.7492 - val_loss: 1.5004 - val_accuracy: 0.6322\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1653 - accuracy: 0.7449 - val_loss: 2.0188 - val_accuracy: 0.5609\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1644 - accuracy: 0.7478 - val_loss: 1.5429 - val_accuracy: 0.6109\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1481 - accuracy: 0.7496 - val_loss: 1.4413 - val_accuracy: 0.6490\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1545 - accuracy: 0.7477 - val_loss: 1.2270 - val_accuracy: 0.7166\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1868 - accuracy: 0.7395 - val_loss: 2.2263 - val_accuracy: 0.5089\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1864 - accuracy: 0.7389 - val_loss: 1.6662 - val_accuracy: 0.5810\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1579 - accuracy: 0.7496 - val_loss: 1.5636 - val_accuracy: 0.6453\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1522 - accuracy: 0.7497 - val_loss: 1.9300 - val_accuracy: 0.5232\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1598 - accuracy: 0.7457 - val_loss: 1.5035 - val_accuracy: 0.6442\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1560 - accuracy: 0.7445 - val_loss: 2.3077 - val_accuracy: 0.4687\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1505 - accuracy: 0.7544 - val_loss: 1.6295 - val_accuracy: 0.6045\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1531 - accuracy: 0.7524 - val_loss: 1.3072 - val_accuracy: 0.6951\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1510 - accuracy: 0.7523 - val_loss: 1.7237 - val_accuracy: 0.5753\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1606 - accuracy: 0.7474 - val_loss: 2.8044 - val_accuracy: 0.4184\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1654 - accuracy: 0.7504 - val_loss: 2.4479 - val_accuracy: 0.4639\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1587 - accuracy: 0.7475 - val_loss: 1.2538 - val_accuracy: 0.7211\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1611 - accuracy: 0.7491 - val_loss: 1.9585 - val_accuracy: 0.5054\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1511 - accuracy: 0.7526 - val_loss: 1.6132 - val_accuracy: 0.6141\n",
      "Epoch 84/200\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1679 - accuracy: 0.7488 - val_loss: 1.5280 - val_accuracy: 0.6235\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1626 - accuracy: 0.7515 - val_loss: 1.8881 - val_accuracy: 0.5289\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1688 - accuracy: 0.7464 - val_loss: 1.6928 - val_accuracy: 0.5867\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1507 - accuracy: 0.7542 - val_loss: 2.2147 - val_accuracy: 0.4948\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1657 - accuracy: 0.7500 - val_loss: 1.3178 - val_accuracy: 0.6917\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1543 - accuracy: 0.7517 - val_loss: 1.4204 - val_accuracy: 0.6642\n",
      "Epoch 90/200\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1622 - accuracy: 0.7481 - val_loss: 1.9849 - val_accuracy: 0.5377\n",
      "Epoch 91/200\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1692 - accuracy: 0.7489 - val_loss: 1.4891 - val_accuracy: 0.6356\n",
      "Epoch 92/200\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1691 - accuracy: 0.7510 - val_loss: 1.6946 - val_accuracy: 0.5682\n",
      "Epoch 93/200\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1650 - accuracy: 0.7486 - val_loss: 1.4945 - val_accuracy: 0.6412\n",
      "Epoch 94/200\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1645 - accuracy: 0.7481 - val_loss: 1.7312 - val_accuracy: 0.5801\n",
      "Epoch 95/200\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1637 - accuracy: 0.7514 - val_loss: 1.8002 - val_accuracy: 0.5540\n",
      "Epoch 96/200\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1561 - accuracy: 0.7529 - val_loss: 1.4001 - val_accuracy: 0.6630\n",
      "Epoch 97/200\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1576 - accuracy: 0.7513 - val_loss: 1.4885 - val_accuracy: 0.6523\n",
      "Epoch 98/200\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1575 - accuracy: 0.7553 - val_loss: 2.4071 - val_accuracy: 0.4345\n",
      "Epoch 99/200\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 1.1621 - accuracy: 0.7498 - val_loss: 2.9200 - val_accuracy: 0.4475\n",
      "Epoch 100/200\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 1.1093 - accuracy: 0.7708 - val_loss: 0.9006 - val_accuracy: 0.8297\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.9045 - accuracy: 0.8275 - val_loss: 0.8906 - val_accuracy: 0.8256\n",
      "Epoch 102/200\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.8331 - accuracy: 0.8448 - val_loss: 0.8739 - val_accuracy: 0.8266\n",
      "Epoch 103/200\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.7872 - accuracy: 0.8507 - val_loss: 0.7591 - val_accuracy: 0.8547\n",
      "Epoch 104/200\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.7541 - accuracy: 0.8560 - val_loss: 0.8205 - val_accuracy: 0.8297\n",
      "Epoch 105/200\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.7214 - accuracy: 0.8587 - val_loss: 0.7356 - val_accuracy: 0.8489\n",
      "Epoch 106/200\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.7027 - accuracy: 0.8586 - val_loss: 0.7873 - val_accuracy: 0.8317\n",
      "Epoch 107/200\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.6694 - accuracy: 0.8669 - val_loss: 0.7208 - val_accuracy: 0.8440\n",
      "Epoch 108/200\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.6559 - accuracy: 0.8648 - val_loss: 0.6956 - val_accuracy: 0.8489\n",
      "Epoch 109/200\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.6399 - accuracy: 0.8668 - val_loss: 0.8079 - val_accuracy: 0.8151\n",
      "Epoch 110/200\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.6239 - accuracy: 0.8707 - val_loss: 0.7478 - val_accuracy: 0.8301\n",
      "Epoch 111/200\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.6146 - accuracy: 0.8719 - val_loss: 0.6790 - val_accuracy: 0.8423\n",
      "Epoch 112/200\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5957 - accuracy: 0.8721 - val_loss: 0.7080 - val_accuracy: 0.8370\n",
      "Epoch 113/200\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5865 - accuracy: 0.8748 - val_loss: 0.6608 - val_accuracy: 0.8539\n",
      "Epoch 114/200\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5841 - accuracy: 0.8759 - val_loss: 0.7026 - val_accuracy: 0.8327\n",
      "Epoch 115/200\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5839 - accuracy: 0.8729 - val_loss: 0.7963 - val_accuracy: 0.8133\n",
      "Epoch 116/200\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5792 - accuracy: 0.8735 - val_loss: 0.6391 - val_accuracy: 0.8540\n",
      "Epoch 117/200\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5742 - accuracy: 0.8728 - val_loss: 0.7406 - val_accuracy: 0.8249\n",
      "Epoch 118/200\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5678 - accuracy: 0.8770 - val_loss: 0.6874 - val_accuracy: 0.8395\n",
      "Epoch 119/200\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5699 - accuracy: 0.8744 - val_loss: 0.6360 - val_accuracy: 0.8504\n",
      "Epoch 120/200\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5649 - accuracy: 0.8757 - val_loss: 0.6386 - val_accuracy: 0.8530\n",
      "Epoch 121/200\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5602 - accuracy: 0.8780 - val_loss: 0.7300 - val_accuracy: 0.8268\n",
      "Epoch 122/200\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5580 - accuracy: 0.8776 - val_loss: 0.6249 - val_accuracy: 0.8524\n",
      "Epoch 123/200\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5571 - accuracy: 0.8793 - val_loss: 0.7122 - val_accuracy: 0.8324\n",
      "Epoch 124/200\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5585 - accuracy: 0.8760 - val_loss: 0.7141 - val_accuracy: 0.8313\n",
      "Epoch 125/200\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5670 - accuracy: 0.8727 - val_loss: 0.7085 - val_accuracy: 0.8266\n",
      "Epoch 126/200\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5571 - accuracy: 0.8773 - val_loss: 0.7517 - val_accuracy: 0.8185\n",
      "Epoch 127/200\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5573 - accuracy: 0.8768 - val_loss: 0.6243 - val_accuracy: 0.8556\n",
      "Epoch 128/200\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5593 - accuracy: 0.8779 - val_loss: 0.6371 - val_accuracy: 0.8517\n",
      "Epoch 129/200\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5554 - accuracy: 0.8785 - val_loss: 0.6670 - val_accuracy: 0.8444\n",
      "Epoch 130/200\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5539 - accuracy: 0.8798 - val_loss: 0.7582 - val_accuracy: 0.8194\n",
      "Epoch 131/200\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5524 - accuracy: 0.8788 - val_loss: 0.8694 - val_accuracy: 0.7920\n",
      "Epoch 132/200\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5482 - accuracy: 0.8823 - val_loss: 0.6937 - val_accuracy: 0.8412\n",
      "Epoch 133/200\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5545 - accuracy: 0.8802 - val_loss: 0.7310 - val_accuracy: 0.8260\n",
      "Epoch 134/200\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5470 - accuracy: 0.8838 - val_loss: 0.6728 - val_accuracy: 0.8425\n",
      "Epoch 135/200\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5431 - accuracy: 0.8834 - val_loss: 0.7949 - val_accuracy: 0.8126\n",
      "Epoch 136/200\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5478 - accuracy: 0.8823 - val_loss: 0.6781 - val_accuracy: 0.8420\n",
      "Epoch 137/200\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5463 - accuracy: 0.8832 - val_loss: 0.8050 - val_accuracy: 0.8041\n",
      "Epoch 138/200\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5442 - accuracy: 0.8846 - val_loss: 0.6941 - val_accuracy: 0.8354\n",
      "Epoch 139/200\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5613 - accuracy: 0.8804 - val_loss: 0.6542 - val_accuracy: 0.8495\n",
      "Epoch 140/200\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5533 - accuracy: 0.8825 - val_loss: 0.7349 - val_accuracy: 0.8276\n",
      "Epoch 141/200\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5441 - accuracy: 0.8859 - val_loss: 0.6927 - val_accuracy: 0.8424\n",
      "Epoch 142/200\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5570 - accuracy: 0.8828 - val_loss: 0.7137 - val_accuracy: 0.8340\n",
      "Epoch 143/200\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5500 - accuracy: 0.8835 - val_loss: 0.6923 - val_accuracy: 0.8439\n",
      "Epoch 144/200\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5464 - accuracy: 0.8868 - val_loss: 0.6994 - val_accuracy: 0.8410\n",
      "Epoch 145/200\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5425 - accuracy: 0.8872 - val_loss: 0.7280 - val_accuracy: 0.8289\n",
      "Epoch 146/200\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5465 - accuracy: 0.8862 - val_loss: 0.7774 - val_accuracy: 0.8189\n",
      "Epoch 147/200\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5455 - accuracy: 0.8853 - val_loss: 0.6974 - val_accuracy: 0.8353\n",
      "Epoch 148/200\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5534 - accuracy: 0.8841 - val_loss: 0.6842 - val_accuracy: 0.8447\n",
      "Epoch 149/200\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5465 - accuracy: 0.8886 - val_loss: 0.8604 - val_accuracy: 0.7903\n",
      "Epoch 150/200\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.5164 - accuracy: 0.8989 - val_loss: 0.5406 - val_accuracy: 0.8921\n",
      "Epoch 151/200\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4529 - accuracy: 0.9194 - val_loss: 0.5301 - val_accuracy: 0.8940\n",
      "Epoch 152/200\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4384 - accuracy: 0.9236 - val_loss: 0.5299 - val_accuracy: 0.8963\n",
      "Epoch 153/200\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4296 - accuracy: 0.9256 - val_loss: 0.5399 - val_accuracy: 0.8918\n",
      "Epoch 154/200\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4148 - accuracy: 0.9304 - val_loss: 0.5278 - val_accuracy: 0.8982\n",
      "Epoch 155/200\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4068 - accuracy: 0.9317 - val_loss: 0.5159 - val_accuracy: 0.9005\n",
      "Epoch 156/200\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4015 - accuracy: 0.9342 - val_loss: 0.5136 - val_accuracy: 0.8988\n",
      "Epoch 157/200\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3996 - accuracy: 0.9332 - val_loss: 0.5230 - val_accuracy: 0.8993\n",
      "Epoch 158/200\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3926 - accuracy: 0.9351 - val_loss: 0.5152 - val_accuracy: 0.9008\n",
      "Epoch 159/200\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3920 - accuracy: 0.9351 - val_loss: 0.5153 - val_accuracy: 0.9031\n",
      "Epoch 160/200\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3832 - accuracy: 0.9380 - val_loss: 0.5167 - val_accuracy: 0.8993\n",
      "Epoch 161/200\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3802 - accuracy: 0.9380 - val_loss: 0.5220 - val_accuracy: 0.8985\n",
      "Epoch 162/200\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3724 - accuracy: 0.9396 - val_loss: 0.5117 - val_accuracy: 0.9008\n",
      "Epoch 163/200\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3703 - accuracy: 0.9408 - val_loss: 0.5079 - val_accuracy: 0.9028\n",
      "Epoch 164/200\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3671 - accuracy: 0.9397 - val_loss: 0.5053 - val_accuracy: 0.9039\n",
      "Epoch 165/200\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3591 - accuracy: 0.9430 - val_loss: 0.4978 - val_accuracy: 0.9061\n",
      "Epoch 166/200\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3588 - accuracy: 0.9436 - val_loss: 0.5063 - val_accuracy: 0.9024\n",
      "Epoch 167/200\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3513 - accuracy: 0.9454 - val_loss: 0.4919 - val_accuracy: 0.9060\n",
      "Epoch 168/200\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3467 - accuracy: 0.9463 - val_loss: 0.5162 - val_accuracy: 0.9005\n",
      "Epoch 169/200\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3469 - accuracy: 0.9457 - val_loss: 0.5005 - val_accuracy: 0.9051\n",
      "Epoch 170/200\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3466 - accuracy: 0.9462 - val_loss: 0.4873 - val_accuracy: 0.9055\n",
      "Epoch 171/200\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3391 - accuracy: 0.9470 - val_loss: 0.5212 - val_accuracy: 0.8983\n",
      "Epoch 172/200\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3411 - accuracy: 0.9466 - val_loss: 0.5064 - val_accuracy: 0.9013\n",
      "Epoch 173/200\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3334 - accuracy: 0.9479 - val_loss: 0.5078 - val_accuracy: 0.9049\n",
      "Epoch 174/200\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3366 - accuracy: 0.9459 - val_loss: 0.5171 - val_accuracy: 0.9000\n",
      "Epoch 175/200\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3351 - accuracy: 0.9460 - val_loss: 0.4892 - val_accuracy: 0.9065\n",
      "Epoch 176/200\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3223 - accuracy: 0.9521 - val_loss: 0.5017 - val_accuracy: 0.9033\n",
      "Epoch 177/200\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3349 - accuracy: 0.9477 - val_loss: 0.4958 - val_accuracy: 0.9046\n",
      "Epoch 178/200\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3269 - accuracy: 0.9471 - val_loss: 0.4918 - val_accuracy: 0.9050\n",
      "Epoch 179/200\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3127 - accuracy: 0.9534 - val_loss: 0.4882 - val_accuracy: 0.9055\n",
      "Epoch 180/200\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3174 - accuracy: 0.9516 - val_loss: 0.5097 - val_accuracy: 0.9025\n",
      "Epoch 181/200\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3216 - accuracy: 0.9496 - val_loss: 0.5097 - val_accuracy: 0.9010\n",
      "Epoch 182/200\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3081 - accuracy: 0.9531 - val_loss: 0.5104 - val_accuracy: 0.9018\n",
      "Epoch 183/200\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3136 - accuracy: 0.9517 - val_loss: 0.4992 - val_accuracy: 0.9031\n",
      "Epoch 184/200\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3071 - accuracy: 0.9532 - val_loss: 0.5223 - val_accuracy: 0.8989\n",
      "Epoch 185/200\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3034 - accuracy: 0.9551 - val_loss: 0.4973 - val_accuracy: 0.9047\n",
      "Epoch 186/200\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3058 - accuracy: 0.9520 - val_loss: 0.5095 - val_accuracy: 0.8990\n",
      "Epoch 187/200\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2980 - accuracy: 0.9550 - val_loss: 0.4971 - val_accuracy: 0.9039\n",
      "Epoch 188/200\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3018 - accuracy: 0.9553 - val_loss: 0.5029 - val_accuracy: 0.8990\n",
      "Epoch 189/200\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2963 - accuracy: 0.9549 - val_loss: 0.4933 - val_accuracy: 0.9053\n",
      "Epoch 190/200\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2963 - accuracy: 0.9542 - val_loss: 0.4961 - val_accuracy: 0.9049\n",
      "Epoch 191/200\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2957 - accuracy: 0.9560 - val_loss: 0.5061 - val_accuracy: 0.9026\n",
      "Epoch 192/200\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2907 - accuracy: 0.9564 - val_loss: 0.5035 - val_accuracy: 0.9024\n",
      "Epoch 193/200\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2918 - accuracy: 0.9551 - val_loss: 0.5120 - val_accuracy: 0.8986\n",
      "Epoch 194/200\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2862 - accuracy: 0.9567 - val_loss: 0.4936 - val_accuracy: 0.9015\n",
      "Epoch 195/200\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2877 - accuracy: 0.9558 - val_loss: 0.5384 - val_accuracy: 0.8926\n",
      "Epoch 196/200\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2886 - accuracy: 0.9555 - val_loss: 0.5142 - val_accuracy: 0.8967\n",
      "Epoch 197/200\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2820 - accuracy: 0.9565 - val_loss: 0.5092 - val_accuracy: 0.8979\n",
      "Epoch 198/200\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2800 - accuracy: 0.9591 - val_loss: 0.4961 - val_accuracy: 0.9029\n",
      "Epoch 199/200\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2781 - accuracy: 0.9596 - val_loss: 0.5042 - val_accuracy: 0.9019\n",
      "Epoch 200/200\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2792 - accuracy: 0.9574 - val_loss: 0.5231 - val_accuracy: 0.8933\n"
     ]
    }
   ],
   "source": [
    "odin_rn_model.compile(loss=loss_fn, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = odin_rn_model.fit(train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "QPWAgdc65Ppc",
    "outputId": "24617f95-2bf1-4e61-a469-ae0589a4441d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: odin_rn_model/assets\n"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"test loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "odin_rn_model.save(\"odin_rn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 97.05%\n",
      "Test accuracy: 89.33%\n"
     ]
    }
   ],
   "source": [
    "_, train_acc = odin_rn_model.evaluate(train_ds, verbose=0)\n",
    "_, test_acc = odin_rn_model.evaluate(test_ds, verbose=0)\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOilU58o+F3Cx1887JA7Q+B",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "mount_file_id": "https://github.com/sayakpaul/Generalized-ODIN-TF/blob/main/Generalized_ODIN.ipynb",
   "name": "Generalized_ODIN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
