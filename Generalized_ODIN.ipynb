{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayakpaul/Generalized-ODIN-TF/blob/main/Generalized_ODIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFwWgoioRXy9"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCQ1S81KIbaN",
    "outputId": "5a9b4b37-56fd-4665-b754-a0e267a094d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User name: sayakpaul\n",
      "Password: ··········\n",
      "Repo Address: sayakpaul/Generalized-ODIN-TF\n",
      "Branch name: main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "from getpass import getpass\n",
    "\n",
    "user = input('User name: ')\n",
    "password = getpass('Password: ')\n",
    "password = urllib.parse.quote(password)\n",
    "repo_address = input('Repo Address: ')\n",
    "branch_name = input('Branch name: ')\n",
    "\n",
    "cmd_string = 'git clone https://{}:{}@github.com/{}.git -b {}'.format(\n",
    "    user, password, repo_address, branch_name\n",
    ")\n",
    "\n",
    "os.system(cmd_string)\n",
    "cmd_string, password = \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QtwAr7vgIpS5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"Generalized-ODIN-TF\")\n",
    "\n",
    "import resnet20_odin\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWpj-WTpRZvS"
   },
   "source": [
    "## Load CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6mPJL_5JDsO",
    "outputId": "500c0fc7-dd46-45ae-9245-c2ae4ef6111a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 50000\n",
      "Total test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "print(f\"Total training examples: {len(x_train)}\")\n",
    "print(f\"Total test examples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMIkjg6_RbYc"
   },
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5A1ZdFW6J4_R"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "START_LR = 0.1\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtQQLiGNRdvx"
   },
   "source": [
    "## Prepare data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GqNtOj8yKO_X"
   },
   "outputs": [],
   "source": [
    "# Augmentation pipeline\n",
    "simple_aug = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
    "        layers.experimental.preprocessing.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now, map the augmentation pipeline to our training dataset\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(lambda x, y: (simple_aug(x), y), num_parallel_calls=AUTO)\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JOCwKSQRfvl"
   },
   "source": [
    "## Utility function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ssNNrdG7K2bw"
   },
   "outputs": [],
   "source": [
    "def get_rn_model(num_classes=10):\n",
    "    n = 2\n",
    "    depth = n * 9 + 2\n",
    "    n_blocks = ((depth - 2) // 9) - 1\n",
    "\n",
    "    # The input tensor\n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    x = layers.experimental.preprocessing.Rescaling(scale=1.0 / 127.5, offset=-1)(\n",
    "        inputs\n",
    "    )\n",
    "\n",
    "    # The Stem Convolution Group\n",
    "    x = resnet20_odin.stem(x)\n",
    "\n",
    "    # The learner\n",
    "    x = resnet20_odin.learner(x, n_blocks)\n",
    "\n",
    "    # The Classifier for 10 classes\n",
    "    outputs = resnet20_odin.classifier(x, num_classes)\n",
    "\n",
    "    # Instantiate the Model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXUFugU3Riph"
   },
   "source": [
    "## Define LR schedule, optimizer, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8a6VIseo7GFP"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    if epoch < int(EPOCHS * 0.5) - 1:\n",
    "        return START_LR\n",
    "    elif epoch < int(EPOCHS*0.75) -1:\n",
    "        return float(START_LR * 0.1)\n",
    "    else:\n",
    "        return float(START_LR * 0.01)\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr_schedule(epoch), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iUhtb9ZiM17B"
   },
   "outputs": [],
   "source": [
    "# Optimizer and loss function.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=START_LR, momentum=0.9)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkU9afGCRnaC"
   },
   "source": [
    "## Model training with ResNet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3zhJ1iFPLzp",
    "outputId": "a858aef4-4c17-4b90-cc10-413707d2cbff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 16s 33ms/step - loss: 8.1664\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 4.9656\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 3.4281\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 2.6990\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 2.2887\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 2.0135\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.8412\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.7513\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.7150\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.7795\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.5873\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.5046\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.4559\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.4105\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3859\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.4069\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3687\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.8315\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.9784\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.5624\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.4208\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3633\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3313\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3621\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3064\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.2998\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3011\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3100\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 3.5881\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 3.9504\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 21.1844\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 10.5316\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 5.9124\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 3.7521\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 2.7540\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 2.2762\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 2.0224\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.9002\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.7772\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.7147\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.7564\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.6324\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.5979\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.5623\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.5354\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.5194\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.4907\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.4613\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.4432\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.4207\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.4100\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 2.2666\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 2.9210\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.9234\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.6212\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.5048\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.4287\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3986\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.4008\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3831\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3855\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3697\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3650\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3729\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3538\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3650\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3541\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3499\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3477\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3521\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3660\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3532\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3521\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3503\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3443\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3228\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3479\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3341\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3423\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3357\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3368\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3558\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3450\n",
      "Epoch 84/200\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3316\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3400\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3369\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3365\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3451\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3351\n",
      "Epoch 90/200\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3330\n",
      "Epoch 91/200\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3356\n",
      "Epoch 92/200\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3681\n",
      "Epoch 93/200\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3405\n",
      "Epoch 94/200\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3425\n",
      "Epoch 95/200\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3233\n",
      "Epoch 96/200\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3424\n",
      "Epoch 97/200\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3339\n",
      "Epoch 98/200\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3274\n",
      "Epoch 99/200\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.3416\n",
      "Epoch 100/200\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.2711\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.0540\n",
      "Epoch 102/200\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.9879\n",
      "Epoch 103/200\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.9449\n",
      "Epoch 104/200\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.9139\n",
      "Epoch 105/200\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.8757\n",
      "Epoch 106/200\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.8640\n",
      "Epoch 107/200\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.8274\n",
      "Epoch 108/200\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.8232\n",
      "Epoch 109/200\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7941\n",
      "Epoch 110/200\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7808\n",
      "Epoch 111/200\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7676\n",
      "Epoch 112/200\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7679\n",
      "Epoch 113/200\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7591\n",
      "Epoch 114/200\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7552\n",
      "Epoch 115/200\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7496\n",
      "Epoch 116/200\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7446\n",
      "Epoch 117/200\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7358\n",
      "Epoch 118/200\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7321\n",
      "Epoch 119/200\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7364\n",
      "Epoch 120/200\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7350\n",
      "Epoch 121/200\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7269\n",
      "Epoch 122/200\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7243\n",
      "Epoch 123/200\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7336\n",
      "Epoch 124/200\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7189\n",
      "Epoch 125/200\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7293\n",
      "Epoch 126/200\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7185\n",
      "Epoch 127/200\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7180\n",
      "Epoch 128/200\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7140\n",
      "Epoch 129/200\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7110\n",
      "Epoch 130/200\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7167\n",
      "Epoch 131/200\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.6999\n",
      "Epoch 132/200\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7056\n",
      "Epoch 133/200\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7106\n",
      "Epoch 134/200\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7033\n",
      "Epoch 135/200\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.6968\n",
      "Epoch 136/200\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7120\n",
      "Epoch 137/200\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7102\n",
      "Epoch 138/200\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7099\n",
      "Epoch 139/200\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7112\n",
      "Epoch 140/200\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7064\n",
      "Epoch 141/200\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7009\n",
      "Epoch 142/200\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7063\n",
      "Epoch 143/200\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6981\n",
      "Epoch 144/200\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.7123\n",
      "Epoch 145/200\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7040\n",
      "Epoch 146/200\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.6904\n",
      "Epoch 147/200\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7026\n",
      "Epoch 148/200\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.6985\n",
      "Epoch 149/200\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7086\n",
      "Epoch 150/200\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.6569\n",
      "Epoch 151/200\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5930\n",
      "Epoch 152/200\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5774\n",
      "Epoch 153/200\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5623\n",
      "Epoch 154/200\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5487\n",
      "Epoch 155/200\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5440\n",
      "Epoch 156/200\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5320\n",
      "Epoch 157/200\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5285\n",
      "Epoch 158/200\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5196\n",
      "Epoch 159/200\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5228\n",
      "Epoch 160/200\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5125\n",
      "Epoch 161/200\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5058\n",
      "Epoch 162/200\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5040\n",
      "Epoch 163/200\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4947\n",
      "Epoch 164/200\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4969\n",
      "Epoch 165/200\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4863\n",
      "Epoch 166/200\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4907\n",
      "Epoch 167/200\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4754\n",
      "Epoch 168/200\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4738\n",
      "Epoch 169/200\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4766\n",
      "Epoch 170/200\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4686\n",
      "Epoch 171/200\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4641\n",
      "Epoch 172/200\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4660\n",
      "Epoch 173/200\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4606\n",
      "Epoch 174/200\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4624\n",
      "Epoch 175/200\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4548\n",
      "Epoch 176/200\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4567\n",
      "Epoch 177/200\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4574\n",
      "Epoch 178/200\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4501\n",
      "Epoch 179/200\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4432\n",
      "Epoch 180/200\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4474\n",
      "Epoch 181/200\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4421\n",
      "Epoch 182/200\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4351\n",
      "Epoch 183/200\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4344\n",
      "Epoch 184/200\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4281\n",
      "Epoch 185/200\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4315\n",
      "Epoch 186/200\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4299\n",
      "Epoch 187/200\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4151\n",
      "Epoch 188/200\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4207\n",
      "Epoch 189/200\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4185\n",
      "Epoch 190/200\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4133\n",
      "Epoch 191/200\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4168\n",
      "Epoch 192/200\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4155\n",
      "Epoch 193/200\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4089\n",
      "Epoch 194/200\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4039\n",
      "Epoch 195/200\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4133\n",
      "Epoch 196/200\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4115\n",
      "Epoch 197/200\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.3972\n",
      "Epoch 198/200\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.3989\n",
      "Epoch 199/200\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4014\n",
      "Epoch 200/200\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.3995\n"
     ]
    }
   ],
   "source": [
    "odin_rn_model = get_rn_model()\n",
    "odin_rn_model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "history = odin_rn_model.fit(train_ds,\n",
    "               epochs=EPOCHS,\n",
    "               callbacks=[lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "QPWAgdc65Ppc",
    "outputId": "24617f95-2bf1-4e61-a469-ae0589a4441d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: odin_rn_model/assets\n"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "odin_rn_model.save(\"odin_rn_model\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOilU58o+F3Cx1887JA7Q+B",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "mount_file_id": "https://github.com/sayakpaul/Generalized-ODIN-TF/blob/main/Generalized_ODIN.ipynb",
   "name": "Generalized_ODIN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
