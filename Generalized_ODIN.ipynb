{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generalized_ODIN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1LfVnzUgm8m6JsjeVqp6FD1zEHPPmvVsH",
      "authorship_tag": "ABX9TyPYou979LMOy/WpTCLR/IyU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/Generalized-ODIN-TF/blob/main/Generalized_ODIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFwWgoioRXy9"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCQ1S81KIbaN",
        "outputId": "eaf1f9a3-0590-446e-907e-7bd7fba1ae3e"
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "from getpass import getpass\n",
        "\n",
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password)\n",
        "repo_address = input('Repo Address: ')\n",
        "branch_name = input('Branch name: ')\n",
        "\n",
        "cmd_string = 'git clone https://{}:{}@github.com/{}.git -b {}'.format(\n",
        "    user, password, repo_address, branch_name\n",
        ")\n",
        "\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: sayakpaul\n",
            "Password: ··········\n",
            "Repo Address: sayakpaul/Generalized-ODIN-TF\n",
            "Branch name: main\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtwAr7vgIpS5"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"Generalized-ODIN-TF\")\n",
        "\n",
        "import resnet20, wide_resnet\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWpj-WTpRZvS"
      },
      "source": [
        "## Load CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6mPJL_5JDsO",
        "outputId": "97e609cd-33e5-47c4-ddb4-c94c972c2695"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "print(f\"Total training examples: {len(x_train)}\")\n",
        "print(f\"Total test examples: {len(x_test)}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "Total training examples: 50000\n",
            "Total test examples: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMIkjg6_RbYc"
      },
      "source": [
        "## Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A1ZdFW6J4_R"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 200\n",
        "START_LR = 0.01\n",
        "AUTO = tf.data.AUTOTUNE"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtQQLiGNRdvx"
      },
      "source": [
        "## Prepare data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqNtOj8yKO_X"
      },
      "source": [
        "# Augmentation pipeline\n",
        "simple_aug = tf.keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
        "        layers.experimental.preprocessing.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Now, map the augmentation pipeline to our training dataset\n",
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(BATCH_SIZE * 100)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(lambda x, y: (simple_aug(x), y), num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JOCwKSQRfvl"
      },
      "source": [
        "## Utility function for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssNNrdG7K2bw"
      },
      "source": [
        "def get_rn_model(num_classes=10):\n",
        "    n = 2\n",
        "    depth = n * 9 + 2\n",
        "    n_blocks = ((depth - 2) // 9) - 1\n",
        "\n",
        "    # The input tensor\n",
        "    inputs = layers.Input(shape=(32, 32, 3))\n",
        "    x = layers.experimental.preprocessing.Rescaling(scale=1.0 / 127.5, offset=-1)(\n",
        "        inputs\n",
        "    )\n",
        "\n",
        "    # The Stem Convolution Group\n",
        "    x = resnet20.stem(x)\n",
        "\n",
        "    # The learner\n",
        "    x = resnet20.learner(x, n_blocks)\n",
        "\n",
        "    # The Classifier for 10 classes\n",
        "    outputs = resnet20.classifier(x, num_classes)\n",
        "\n",
        "    # Instantiate the Model\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXUFugU3Riph"
      },
      "source": [
        "## Define LR schedule, optimizer, and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a6VIseo7GFP"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    if epoch < int(EPOCHS * 0.5) - 1:\n",
        "        return START_LR\n",
        "    elif epoch < int(EPOCHS*0.75) -1:\n",
        "        return float(START_LR * 0.1)\n",
        "    else:\n",
        "        return float(START_LR * 0.01)\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr_schedule(epoch), verbose=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUhtb9ZiM17B"
      },
      "source": [
        "# Optimizer and loss function.\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=START_LR, momentum=0.9)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkU9afGCRnaC"
      },
      "source": [
        "## Model training with ResNet20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3zhJ1iFPLzp",
        "outputId": "515dab05-2b3d-4029-ee7d-55a773e4a26a"
      },
      "source": [
        "odin_rn_model = get_rn_model()\n",
        "odin_rn_model.compile(loss=loss_fn, optimizer=optimizer)\n",
        "history = odin_rn_model.fit(train_ds,\n",
        "               epochs=EPOCHS,\n",
        "               callbacks=[lr_callback])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 11s 22ms/step - loss: 3.7343\n",
            "Epoch 2/200\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 8s 22ms/step - loss: 2.8972\n",
            "Epoch 3/200\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 2.6691\n",
            "Epoch 4/200\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 2.4960\n",
            "Epoch 5/200\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 2.3364\n",
            "Epoch 6/200\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 2.1708\n",
            "Epoch 7/200\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 2.0372\n",
            "Epoch 8/200\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.8991\n",
            "Epoch 9/200\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.7972\n",
            "Epoch 10/200\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.6833\n",
            "Epoch 11/200\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.5874\n",
            "Epoch 12/200\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.5039\n",
            "Epoch 13/200\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.4355\n",
            "Epoch 14/200\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.3666\n",
            "Epoch 15/200\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.2978\n",
            "Epoch 16/200\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.2403\n",
            "Epoch 17/200\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.1968\n",
            "Epoch 18/200\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.1455\n",
            "Epoch 19/200\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.0972\n",
            "Epoch 20/200\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.0588\n",
            "Epoch 21/200\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 1.0177\n",
            "Epoch 22/200\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.9932\n",
            "Epoch 23/200\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.9634\n",
            "Epoch 24/200\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.9419\n",
            "Epoch 25/200\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.9119\n",
            "Epoch 26/200\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.8816\n",
            "Epoch 27/200\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.8862\n",
            "Epoch 28/200\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.8587\n",
            "Epoch 29/200\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.8324\n",
            "Epoch 30/200\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.8159\n",
            "Epoch 31/200\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.8081\n",
            "Epoch 32/200\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.7973\n",
            "Epoch 33/200\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.7813\n",
            "Epoch 34/200\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.7674\n",
            "Epoch 35/200\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.7691\n",
            "Epoch 36/200\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.7462\n",
            "Epoch 37/200\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.7308\n",
            "Epoch 38/200\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.7293\n",
            "Epoch 39/200\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.7217\n",
            "Epoch 40/200\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.7163\n",
            "Epoch 41/200\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.7004\n",
            "Epoch 42/200\n",
            "\n",
            "Epoch 00042: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6974\n",
            "Epoch 43/200\n",
            "\n",
            "Epoch 00043: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6839\n",
            "Epoch 44/200\n",
            "\n",
            "Epoch 00044: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6923\n",
            "Epoch 45/200\n",
            "\n",
            "Epoch 00045: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6864\n",
            "Epoch 46/200\n",
            "\n",
            "Epoch 00046: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6835\n",
            "Epoch 47/200\n",
            "\n",
            "Epoch 00047: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6762\n",
            "Epoch 48/200\n",
            "\n",
            "Epoch 00048: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6686\n",
            "Epoch 49/200\n",
            "\n",
            "Epoch 00049: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6624\n",
            "Epoch 50/200\n",
            "\n",
            "Epoch 00050: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6660\n",
            "Epoch 51/200\n",
            "\n",
            "Epoch 00051: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6512\n",
            "Epoch 52/200\n",
            "\n",
            "Epoch 00052: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6464\n",
            "Epoch 53/200\n",
            "\n",
            "Epoch 00053: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6556\n",
            "Epoch 54/200\n",
            "\n",
            "Epoch 00054: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6451\n",
            "Epoch 55/200\n",
            "\n",
            "Epoch 00055: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6425\n",
            "Epoch 56/200\n",
            "\n",
            "Epoch 00056: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6472\n",
            "Epoch 57/200\n",
            "\n",
            "Epoch 00057: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6303\n",
            "Epoch 58/200\n",
            "\n",
            "Epoch 00058: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6385\n",
            "Epoch 59/200\n",
            "\n",
            "Epoch 00059: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6291\n",
            "Epoch 60/200\n",
            "\n",
            "Epoch 00060: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6319\n",
            "Epoch 61/200\n",
            "\n",
            "Epoch 00061: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6334\n",
            "Epoch 62/200\n",
            "\n",
            "Epoch 00062: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6263\n",
            "Epoch 63/200\n",
            "\n",
            "Epoch 00063: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6234\n",
            "Epoch 64/200\n",
            "\n",
            "Epoch 00064: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6158\n",
            "Epoch 65/200\n",
            "\n",
            "Epoch 00065: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6148\n",
            "Epoch 66/200\n",
            "\n",
            "Epoch 00066: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6098\n",
            "Epoch 67/200\n",
            "\n",
            "Epoch 00067: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6135\n",
            "Epoch 68/200\n",
            "\n",
            "Epoch 00068: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6136\n",
            "Epoch 69/200\n",
            "\n",
            "Epoch 00069: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5963\n",
            "Epoch 70/200\n",
            "\n",
            "Epoch 00070: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6089\n",
            "Epoch 71/200\n",
            "\n",
            "Epoch 00071: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6030\n",
            "Epoch 72/200\n",
            "\n",
            "Epoch 00072: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6055\n",
            "Epoch 73/200\n",
            "\n",
            "Epoch 00073: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6054\n",
            "Epoch 74/200\n",
            "\n",
            "Epoch 00074: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5985\n",
            "Epoch 75/200\n",
            "\n",
            "Epoch 00075: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6080\n",
            "Epoch 76/200\n",
            "\n",
            "Epoch 00076: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5973\n",
            "Epoch 77/200\n",
            "\n",
            "Epoch 00077: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.6001\n",
            "Epoch 78/200\n",
            "\n",
            "Epoch 00078: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5810\n",
            "Epoch 79/200\n",
            "\n",
            "Epoch 00079: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5966\n",
            "Epoch 80/200\n",
            "\n",
            "Epoch 00080: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5915\n",
            "Epoch 81/200\n",
            "\n",
            "Epoch 00081: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5878\n",
            "Epoch 82/200\n",
            "\n",
            "Epoch 00082: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5876\n",
            "Epoch 83/200\n",
            "\n",
            "Epoch 00083: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5877\n",
            "Epoch 84/200\n",
            "\n",
            "Epoch 00084: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5844\n",
            "Epoch 85/200\n",
            "\n",
            "Epoch 00085: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5836\n",
            "Epoch 86/200\n",
            "\n",
            "Epoch 00086: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5861\n",
            "Epoch 87/200\n",
            "\n",
            "Epoch 00087: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5918\n",
            "Epoch 88/200\n",
            "\n",
            "Epoch 00088: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5833\n",
            "Epoch 89/200\n",
            "\n",
            "Epoch 00089: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5903\n",
            "Epoch 90/200\n",
            "\n",
            "Epoch 00090: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5874\n",
            "Epoch 91/200\n",
            "\n",
            "Epoch 00091: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5770\n",
            "Epoch 92/200\n",
            "\n",
            "Epoch 00092: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5833\n",
            "Epoch 93/200\n",
            "\n",
            "Epoch 00093: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5873\n",
            "Epoch 94/200\n",
            "\n",
            "Epoch 00094: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5783\n",
            "Epoch 95/200\n",
            "\n",
            "Epoch 00095: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5762\n",
            "Epoch 96/200\n",
            "\n",
            "Epoch 00096: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5694\n",
            "Epoch 97/200\n",
            "\n",
            "Epoch 00097: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5728\n",
            "Epoch 98/200\n",
            "\n",
            "Epoch 00098: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5773\n",
            "Epoch 99/200\n",
            "\n",
            "Epoch 00099: LearningRateScheduler reducing learning rate to 0.01.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5764\n",
            "Epoch 100/200\n",
            "\n",
            "Epoch 00100: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.5371\n",
            "Epoch 101/200\n",
            "\n",
            "Epoch 00101: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.4621\n",
            "Epoch 102/200\n",
            "\n",
            "Epoch 00102: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.4438\n",
            "Epoch 103/200\n",
            "\n",
            "Epoch 00103: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.4321\n",
            "Epoch 104/200\n",
            "\n",
            "Epoch 00104: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.4225\n",
            "Epoch 105/200\n",
            "\n",
            "Epoch 00105: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.4122\n",
            "Epoch 106/200\n",
            "\n",
            "Epoch 00106: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.4032\n",
            "Epoch 107/200\n",
            "\n",
            "Epoch 00107: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3977\n",
            "Epoch 108/200\n",
            "\n",
            "Epoch 00108: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3918\n",
            "Epoch 109/200\n",
            "\n",
            "Epoch 00109: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3881\n",
            "Epoch 110/200\n",
            "\n",
            "Epoch 00110: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3823\n",
            "Epoch 111/200\n",
            "\n",
            "Epoch 00111: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3812\n",
            "Epoch 112/200\n",
            "\n",
            "Epoch 00112: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3708\n",
            "Epoch 113/200\n",
            "\n",
            "Epoch 00113: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3752\n",
            "Epoch 114/200\n",
            "\n",
            "Epoch 00114: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3701\n",
            "Epoch 115/200\n",
            "\n",
            "Epoch 00115: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3642\n",
            "Epoch 116/200\n",
            "\n",
            "Epoch 00116: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3585\n",
            "Epoch 117/200\n",
            "\n",
            "Epoch 00117: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3564\n",
            "Epoch 118/200\n",
            "\n",
            "Epoch 00118: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3548\n",
            "Epoch 119/200\n",
            "\n",
            "Epoch 00119: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3515\n",
            "Epoch 120/200\n",
            "\n",
            "Epoch 00120: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3440\n",
            "Epoch 121/200\n",
            "\n",
            "Epoch 00121: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3422\n",
            "Epoch 122/200\n",
            "\n",
            "Epoch 00122: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3360\n",
            "Epoch 123/200\n",
            "\n",
            "Epoch 00123: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3364\n",
            "Epoch 124/200\n",
            "\n",
            "Epoch 00124: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3353\n",
            "Epoch 125/200\n",
            "\n",
            "Epoch 00125: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3332\n",
            "Epoch 126/200\n",
            "\n",
            "Epoch 00126: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3276\n",
            "Epoch 127/200\n",
            "\n",
            "Epoch 00127: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3211\n",
            "Epoch 128/200\n",
            "\n",
            "Epoch 00128: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3220\n",
            "Epoch 129/200\n",
            "\n",
            "Epoch 00129: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3245\n",
            "Epoch 130/200\n",
            "\n",
            "Epoch 00130: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3150\n",
            "Epoch 131/200\n",
            "\n",
            "Epoch 00131: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3159\n",
            "Epoch 132/200\n",
            "\n",
            "Epoch 00132: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3097\n",
            "Epoch 133/200\n",
            "\n",
            "Epoch 00133: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3129\n",
            "Epoch 134/200\n",
            "\n",
            "Epoch 00134: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3063\n",
            "Epoch 135/200\n",
            "\n",
            "Epoch 00135: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3088\n",
            "Epoch 136/200\n",
            "\n",
            "Epoch 00136: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3021\n",
            "Epoch 137/200\n",
            "\n",
            "Epoch 00137: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3032\n",
            "Epoch 138/200\n",
            "\n",
            "Epoch 00138: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3014\n",
            "Epoch 139/200\n",
            "\n",
            "Epoch 00139: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2962\n",
            "Epoch 140/200\n",
            "\n",
            "Epoch 00140: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2962\n",
            "Epoch 141/200\n",
            "\n",
            "Epoch 00141: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2930\n",
            "Epoch 142/200\n",
            "\n",
            "Epoch 00142: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2934\n",
            "Epoch 143/200\n",
            "\n",
            "Epoch 00143: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2930\n",
            "Epoch 144/200\n",
            "\n",
            "Epoch 00144: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2884\n",
            "Epoch 145/200\n",
            "\n",
            "Epoch 00145: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2880\n",
            "Epoch 146/200\n",
            "\n",
            "Epoch 00146: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2816\n",
            "Epoch 147/200\n",
            "\n",
            "Epoch 00147: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2816\n",
            "Epoch 148/200\n",
            "\n",
            "Epoch 00148: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2843\n",
            "Epoch 149/200\n",
            "\n",
            "Epoch 00149: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2815\n",
            "Epoch 150/200\n",
            "\n",
            "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2780\n",
            "Epoch 151/200\n",
            "\n",
            "Epoch 00151: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2619\n",
            "Epoch 152/200\n",
            "\n",
            "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2624\n",
            "Epoch 153/200\n",
            "\n",
            "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2584\n",
            "Epoch 154/200\n",
            "\n",
            "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2573\n",
            "Epoch 155/200\n",
            "\n",
            "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2524\n",
            "Epoch 156/200\n",
            "\n",
            "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2536\n",
            "Epoch 157/200\n",
            "\n",
            "Epoch 00157: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2551\n",
            "Epoch 158/200\n",
            "\n",
            "Epoch 00158: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2539\n",
            "Epoch 159/200\n",
            "\n",
            "Epoch 00159: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2552\n",
            "Epoch 160/200\n",
            "\n",
            "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2490\n",
            "Epoch 161/200\n",
            "\n",
            "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2564\n",
            "Epoch 162/200\n",
            "\n",
            "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2502\n",
            "Epoch 163/200\n",
            "\n",
            "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2494\n",
            "Epoch 164/200\n",
            "\n",
            "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2497\n",
            "Epoch 165/200\n",
            "\n",
            "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2444\n",
            "Epoch 166/200\n",
            "\n",
            "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2497\n",
            "Epoch 167/200\n",
            "\n",
            "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2480\n",
            "Epoch 168/200\n",
            "\n",
            "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2471\n",
            "Epoch 169/200\n",
            "\n",
            "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2486\n",
            "Epoch 170/200\n",
            "\n",
            "Epoch 00170: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2434\n",
            "Epoch 171/200\n",
            "\n",
            "Epoch 00171: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2484\n",
            "Epoch 172/200\n",
            "\n",
            "Epoch 00172: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2459\n",
            "Epoch 173/200\n",
            "\n",
            "Epoch 00173: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2448\n",
            "Epoch 174/200\n",
            "\n",
            "Epoch 00174: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2413\n",
            "Epoch 175/200\n",
            "\n",
            "Epoch 00175: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2439\n",
            "Epoch 176/200\n",
            "\n",
            "Epoch 00176: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2428\n",
            "Epoch 177/200\n",
            "\n",
            "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2431\n",
            "Epoch 178/200\n",
            "\n",
            "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2421\n",
            "Epoch 179/200\n",
            "\n",
            "Epoch 00179: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2419\n",
            "Epoch 180/200\n",
            "\n",
            "Epoch 00180: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2458\n",
            "Epoch 181/200\n",
            "\n",
            "Epoch 00181: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2411\n",
            "Epoch 182/200\n",
            "\n",
            "Epoch 00182: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2405\n",
            "Epoch 183/200\n",
            "\n",
            "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2380\n",
            "Epoch 184/200\n",
            "\n",
            "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2401\n",
            "Epoch 185/200\n",
            "\n",
            "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2415\n",
            "Epoch 186/200\n",
            "\n",
            "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2422\n",
            "Epoch 187/200\n",
            "\n",
            "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2426\n",
            "Epoch 188/200\n",
            "\n",
            "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2355\n",
            "Epoch 189/200\n",
            "\n",
            "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2397\n",
            "Epoch 190/200\n",
            "\n",
            "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2416\n",
            "Epoch 191/200\n",
            "\n",
            "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2385\n",
            "Epoch 192/200\n",
            "\n",
            "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2344\n",
            "Epoch 193/200\n",
            "\n",
            "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2390\n",
            "Epoch 194/200\n",
            "\n",
            "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2314\n",
            "Epoch 195/200\n",
            "\n",
            "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2369\n",
            "Epoch 196/200\n",
            "\n",
            "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2374\n",
            "Epoch 197/200\n",
            "\n",
            "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2331\n",
            "Epoch 198/200\n",
            "\n",
            "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2367\n",
            "Epoch 199/200\n",
            "\n",
            "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2339\n",
            "Epoch 200/200\n",
            "\n",
            "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "QPWAgdc65Ppc",
        "outputId": "4b246565-b075-4bdc-ccfa-eca5c1d28e48"
      },
      "source": [
        "plt.plot(history.history[\"loss\"])\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "odin_rn_model.save(\"odin_rn_model\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: odin_rn_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}